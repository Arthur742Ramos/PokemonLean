\documentclass[journal]{IEEEtran}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{cite}
\usepackage{url}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{balance}
\usepackage{needspace}

\lstdefinelanguage{Lean}{
  morekeywords={theorem,def,structure,inductive,where,match,with,if,then,else,let,in,fun,forall,exists,by,have,show,sorry,exact,simp,omega,decide,native_decide,intro,apply,rfl,instance,class,abbrev,noncomputable,example,lemma,Prop,Type,Nat,Fin,List,Bool,true,false,And,Or,Not},
  sensitive=true,
  morecomment=[l]{--},
  morestring=[b]"
}

\lstset{
  language=Lean,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue!55!black}\bfseries,
  commentstyle=\color{green!40!black}\itshape,
  stringstyle=\color{orange!50!black},
  frame=single,
  breaklines=true,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny\color{gray},
  xleftmargin=1.5em,
  aboveskip=0.5em,
  belowskip=0.5em,
  literate={·}{{$\cdot$}}1 {←}{{$\leftarrow$}}1 {→}{{$\rightarrow$}}1 {∀}{{$\forall$}}1 {∃}{{$\exists$}}1 {≤}{{$\leq$}}1 {≥}{{$\geq$}}1 {≠}{{$\neq$}}1 {⟨}{{$\langle$}}1 {⟩}{{$\rangle$}}1 {↔}{{$\leftrightarrow$}}1 {∧}{{$\land$}}1 {∨}{{$\lor$}}1 {¬}{{$\lnot$}}1 {λ}{{$\lambda$}}1 {⊢}{{$\vdash$}}1
}

\let\origsection\section
\renewcommand{\section}{\needspace{4\baselineskip}\origsection}
\let\origsubsection\subsection
\renewcommand{\subsection}{\needspace{3\baselineskip}\origsubsection}
\clubpenalty=10000
\widowpenalty=10000
\displaywidowpenalty=10000

\begin{document}

\title{From Rules to Nash Equilibria: Formally Verified\\Game-Theoretic Analysis of a Competitive\\Trading Card Game}

\author{Author names withheld for review}

\maketitle

\begin{abstract}
We present a formally verified analysis of competitive Pok\'emon Trading Card Game metagame dynamics using Lean~4 and real tournament data.
Our formal development spans approximately 30{,}000~lines across 75~Lean files and proves over 2{,}500 theorems without \texttt{sorry}, \texttt{admit}, or custom axioms.
Most of this corpus verifies reusable infrastructure (rules, types, legality, and probability foundations), while approximately 180 theorems directly verify the empirical claims reported in this paper.
On top of this foundation, we analyze Trainer Hill tournament data from January~29 to February~19, 2026 (50+ player events), covering 14 archetypes and a complete 14\,$\times$\,14 matchup matrix.
The central result is a verified popularity paradox: Dragapult Dusknoir is the most played deck (15.5\%) but has only 46.7\% expected win rate against the field, while Grimmsnarl Froslass (5.1\% share) has the highest expected field win rate at 52.7\%.
We further report a linear-programming candidate equilibrium concentrated on Mega Absol Box (approximately 93.2\%) and Dragapult (approximately 6.8\%), verify that this reported profile is a valid mixed strategy, prove replicator dynamics pressure against Dragapult and Ceruledge, and quantify best-of-three amplification effects such as 67.3\%\,$\to$\,74.9\% for Raging Bolt Ogerpon versus Mega Absol Box.
These results demonstrate that machine-checked formal methods can produce empirically grounded and strategically actionable conclusions in modern competitive game ecosystems.
\end{abstract}

\begin{IEEEkeywords}
Formal verification, game theory, trading card games, Nash equilibrium, theorem proving, metagame analysis, replicator dynamics, Lean~4
\end{IEEEkeywords}

%======================================================================
\section{Introduction}
%======================================================================

Tournament outcomes in competitive trading card games (TCGs) are often shaped before round one begins.
Players must make two coupled decisions: how to play each game state and which deck to register.
The first decision is local and tactical; the second is global and game-theoretic.
The pre-tournament deck-selection problem is naturally modeled as a strategic game where payoffs derive from matchup win rates and the population distribution of opponents.

The Pok\'emon TCG is especially suitable for this analysis.
It has a large organized-play ecosystem, clearly defined public rules, and a metagame that evolves quickly enough to produce measurable strategic cycles.
At the same time, the domain is difficult for informal reasoning: hidden information, stochastic effects, and nonlinear tournament incentives make intuition unreliable even for experienced players.

Formal methods offer an attractive remedy.
By encoding game semantics in Lean~4~\cite{moura2021lean} and proving strategic statements directly over exact data representations, we separate factual claims from narrative claims.
A statement in the paper is either derivable from formally checked definitions and theorems or it is excluded.
We build a proof-carrying metagame analytics pipeline where the verified objects are
(i) data representation and ingestion,
(ii) expected-value computations over the field,
(iii) candidate-equilibrium computation, mixed-strategy validity checks, and replicator dynamics statements, and
(iv) tournament-objective transforms (Bo3, Swiss).
The in-game rules formalization serves as supporting infrastructure guaranteeing legality and enabling future counterfactual analysis.

Our empirical foundation is Trainer Hill metagame data, aggregated from Limitless tournament records~\cite{trainerhill2026,limitless2024}, for 50+ player tournaments from January~29 to February~19, 2026.
Within this fixed window, we model the top 14 archetypes and their full pairwise matchup matrix.
The resulting matrix contains enough granularity to support rigorous expected-value computations, equilibrium analysis, and evolutionary dynamics without introducing synthetic assumptions about win rates.

This paper makes four concrete contributions aligned with this thesis.
First, we formalize the rules and legality substrate that supports trustworthy ingestion and future counterfactual analysis.
Second, we encode real metagame data as exact values and verify expected-value computations, including the popularity paradox.
Third, we derive candidate-equilibrium and replicator-dynamics statements over the observed matrix, with explicit separation between mixed-strategy validity checks and full best-response equilibrium certification.
Fourth, we verify tournament-objective transforms from single-game payoffs to best-of-three and Swiss-relevant decision criteria.

A key practical implication follows immediately.
Popularity is not a proxy for optimality.
The most visible deck in the room may be strategically dominated once weighted against the actual field.
Formal verification provides a robust way to expose this mismatch and quantify its consequences for player choice.

The remainder of the paper is organized as follows.
Section~\ref{sec:related} situates the work.
Section~\ref{sec:formalization} presents the Lean model of rules and legality.
Section~\ref{sec:probability} develops probability and resource theory.
Section~\ref{sec:data} details data and measurement.
Section~\ref{sec:paradox} presents the popularity paradox.
Section~\ref{sec:nash} analyzes equilibrium and dynamics.
Section~\ref{sec:tournament} discusses tournament strategy.
Section~\ref{sec:methodology} documents formalization methodology.
Section~\ref{sec:threats} covers validity threats, and Section~\ref{sec:conclusion} concludes.

\subsection{Motivating Tournament Scenario}

Consider a player preparing for a 10-round open tournament.
Community sentiment says Dragapult is ``the deck to beat'' because it is popular, has many practiced pilots, and appears frequently in streamed matches.
An informal decision process often stops here: players mirror the visible metagame and assume that high adoption signals high objective value.

Our data and proofs show why this shortcut fails.
Popularity is an endogenous variable shaped by familiarity, deck cost, and social diffusion.
Expected win rate, in contrast, is a payoff object determined by pairwise matchups weighted by opponent frequencies.
These are related but not equivalent quantities.

Suppose the same player has two candidate decks: Dragapult and Grimmsnarl.
The first has better perceived comfort and social validation; the second has lower adoption but stronger weighted performance in this snapshot.
Without a formal model, one may over-weight convenience and under-weight aggregate matchup structure.
With a formal model, the decision can be recast as a transparent optimization problem with explicit assumptions.

This distinction scales from individuals to populations.
If many players independently adopt ``safe'' visible decks despite negative expected fitness, the field can stabilize in a collectively suboptimal state.
That state is exactly what our popularity paradox theorem captures.
In this sense, the theorem is not merely descriptive; it is diagnostic of strategic inefficiency in ecosystem-level behavior.

The practical value for competitors is immediate.
A formally validated metagame model provides a defensible framework for deciding when to exploit, when to hedge, and when to avoid overreacting to noisy week-to-week narratives.
For researchers, it provides a rare domain where rich empirical data and theorem-prover rigor can be combined in one reproducible pipeline.

%======================================================================
\section{Related Work}\label{sec:related}
%======================================================================

\subsection{Formal Methods and Strategic Games}

Formal reasoning has transformed analysis in several strategic domains.
In classical games, foundational communication and search analyses and later complexity results for generalized chess established the computational stakes of strategic reasoning~\cite{shannon1948mathematical,schaefer1978complexity,fraenkel1981chess}.
In imperfect-information settings such as poker, game-theoretic systems like Cepheus, Libratus, and Pluribus~\cite{bowling2015heads,brown2018superhuman,brown2019pluribus} show that equilibrium reasoning can scale when abstractions are carefully managed.
At larger multi-agent scale, AlphaZero and AlphaStar demonstrate superhuman play in complex domains~\cite{silver2018general,vinyals2019alphastar}.

TCGs are harder in a different way.
Their action spaces and card interactions are highly compositional, and rules often involve exception-heavy textual semantics.
This combination increases the risk of silent modeling errors when analysis is implemented in ad hoc scripts.
A proof assistant mitigates this risk by forcing explicit treatment of definitions, invariants, and edge cases.

\subsection{AI and Metagame Analysis in Card Games}

Prior card-game AI work has emphasized in-game decision quality, including Monte Carlo methods for games such as Magic and Hearthstone~\cite{cowling2012information,ward2009monte,santos2017monte,zhang2017deck}.
Related work also includes deckbuilding optimization and competition environments for TCG AI~\cite{bjorke2017deckbuilding,dockhorn2019hearthstone,kowalski2020summon}.
That line of work is valuable but orthogonal to the question we study: how should a player choose a deck before round one, given a population distribution and matchup matrix?

Metagame-level analysis appears frequently in practitioner content but rarely as reproducible formal scholarship.
The missing ingredients are a fixed data definition, a mathematically explicit payoff model, and mechanically checked claims.
Our approach fills this gap by treating metagame analysis as theorem proving over empirical constants.

\subsection{Theorem Proving for Rule Systems}

Lean~4~\cite{moura2021lean} has become a practical environment for large-scale formalization because it combines expressive dependent types with efficient decision procedures.
Large collaborative libraries and landmark formal proofs further show the maturity of this ecosystem~\cite{gonthier2008four,Avigad2007,mathlib2020,hales2017kepler}.
For game-rule modeling, dependent types are particularly useful: constraints such as bounded bench size, deck legality predicates, and well-formed transitions can be encoded directly as propositions attached to data.

Closest in spirit is prior formalization work on card-game effects in Isabelle/HOL~\cite{hosch2022hearthstone}, which established feasibility.
Our work differs by coupling rule formalization to a complete, real matchup matrix and then pushing through equilibrium and dynamics claims tied to observed tournament distributions.

\subsection{Evolutionary and Behavioral Perspectives}

Replicator dynamics~\cite{smith1973logic,taylor1978evolutionary,weibull1997evolutionary} provide a natural lens for metagame adaptation: strategies with above-average fitness gain share, while below-average strategies lose share.
In practice, observed metagames often drift slowly because human behavior is not perfectly rational.
This motivates interpreting deviations from equilibrium in behavioral-economic terms rather than treating them as model failure.

The contribution here is methodological: we use evolutionary tools not as informal metaphors but as theorem-backed statements over fixed data.
When we claim that Dragapult has negative relative fitness or that Ceruledge declines under replicator updates, those claims are machine-checked consequences of the encoded matrix and shares.

\subsection{Community Analytics Versus Proof-Carrying Analytics}

Competitive communities already produce large volumes of metagame commentary.
Those outputs are valuable for speed, but they typically combine raw percentages, subjective confidence, and hand-waved matchup transitivity.
This workflow is well suited for rapid iteration and poor at preventing silent arithmetic or modeling mistakes.

Our approach is intentionally stricter.
Every quantity referenced in argument is represented as an explicit program term.
Every strategic claim is either the direct output of a computation over those terms or a theorem whose proof is validated by the Lean kernel.
The cost is additional formalization overhead; the benefit is auditability and long-term reproducibility.

This positioning is complementary rather than adversarial.
Community analytics can generate hypotheses quickly.
Formal analytics can then validate, reject, or refine those hypotheses with machine-checkable guarantees.
In practice, the two pipelines can coexist: empirical dashboards provide breadth, while theorem-prover workflows provide depth and reliability for high-impact claims.
A Python script can reproduce percentages quickly, but it cannot enforce theorem-level linkage between assumptions, computations, and manuscript claims; the Lean workflow makes those links explicit and checkable.

The popularity paradox is a good example.
Informally, one might suspect overplay from anecdotal matchup frustration.
Formally, we can prove the exact inequality against normalized field weights and make all assumptions explicit.
That transformation from intuition to theorem is the central methodological gap this paper addresses.
Section~\ref{sec:formalization} now defines the formal rule substrate used by the subsequent empirical and strategic analyses.

%======================================================================
\section{Game Formalization}\label{sec:formalization}
%======================================================================

We formalize the strategic layer of the Pok\'emon TCG by combining operational state semantics with legality and resource invariants, grounded in official rule documents~\cite{ptcg_rules,playpokemon2024rules}.
In this paper, the rules layer is supporting infrastructure rather than the primary empirical claim object for the 2026 snapshot.
It future-proofs the framework for counterfactual analysis (e.g., ``what if a card is banned?''), guarantees data-ingestion correctness through legality alignment, and enables mechanical derivation of matchup implications from legal state transitions.
The 2026 snapshot analysis itself relies primarily on matrix-level verification over observed matchup data.
The goal is not to mechanize every card text in existence; instead, we encode the rule substrate needed to reason about deck legality, turn progression, card flow, and payoff-relevant mechanics.

\subsection{Game State Representation}

The state model explicitly tracks per-player zones, turn ownership, and phase.
This representation is sufficient to express legality constraints and to prove conservation and progress properties.

\begin{lstlisting}
structure GameState where
  activeP1   : Pokemon
  benchP1    : List Pokemon
  handP1     : List Card
  deckP1     : List Card
  prizesP1   : List Card
  discardP1  : List Card
  activeP2   : Pokemon
  benchP2    : List Pokemon
  handP2     : List Card
  deckP2     : List Card
  prizesP2   : List Card
  discardP2  : List Card
  turnPlayer : Player
  turnPhase  : Phase
\end{lstlisting}

A state-level encoding gives us a uniform target for rules, tactics, and metatheory.
In particular, all strategic claims can be connected to primitive transition semantics, preventing the common mismatch where high-level analysis assumes mechanics that are absent from the underlying rules implementation.

\subsection{Turn Phases and Transition Discipline}

Turn order is represented as a finite phase machine.
This makes ``what can happen next'' decidable, enabling automation for many local proofs.

\begin{lstlisting}
inductive Phase where
  | DrawPhase
  | MainPhase
  | AttackPhase
  | BetweenTurns
  deriving DecidableEq, Repr
\end{lstlisting}

By constraining transitions through this type, we can prove phase safety lemmas (e.g., no attacks during draw phase) and derive stronger progress claims.
These lemmas are not just implementation detail: they ensure that all strategy-level simulations are grounded in legal game trajectories.

\subsection{Type Effectiveness}

Weakness and resistance are encoded as total functions over enumerated types.
The Grass$\to$Fire interaction appears in the certified weakness-cycle theorem.

\begin{lstlisting}
theorem TRIANGLE :
    Exists fun A : PType =>
      Exists fun B : PType =>
        Exists fun C : PType =>
          weakness A B = true /\ weakness B C = true /\ weakness C A = true := by
  exact Exists.intro PType.grass (Exists.intro PType.fire
    (Exists.intro PType.water (And.intro rfl (And.intro rfl rfl))))
\end{lstlisting}

Even elementary facts matter because they serve as trusted building blocks for larger proofs, especially when computing expected damage and trade sequences.
Encoding them in the core logic avoids accidental divergence between prose assumptions and executable semantics.

\subsection{Card Conservation and Trainer Effects}

We model high-impact trainer cards with explicit zone transitions.
For Professor's Research, we prove that discarding the hand and drawing seven preserves global card count across all zones.

\begin{lstlisting}
theorem professorsResearchEffect_preserves_cards (p : PlayerState) :
    playerCardCount (professorsResearchEffect p) = playerCardCount p := by
  unfold professorsResearchEffect playerCardCount inPlayCount
  simp [List.length_take, List.length_drop, List.length_append]
  omega
\end{lstlisting}

Conservation theorems are critical for trust.
Without them, subtle bookkeeping bugs can distort probability estimates and strategic value calculations.
With them, resource-theoretic statements in later sections inherit mechanical guarantees.

\subsection{Deck Legality as a Biconditional}

Deck legality is implemented as a computable checker and linked to an inductive specification through a soundness-and-completeness theorem.

\begin{lstlisting}
theorem checkDeckLegal_iff (deck : List Card) :
    checkDeckLegal deck = true <-> DeckLegal deck := by
  constructor
  · exact checkDeckLegal_sound deck
  · exact checkDeckLegal_complete deck
\end{lstlisting}

This biconditional is essential for reproducibility.
It guarantees that data ingestion and deck filtering in empirical analysis are extensionally equivalent to the formal legality policy, rather than ``close enough'' approximations.

\subsection{Invariant Catalog and Proof Obligations}

Beyond the highlighted theorems, the formal model maintains a broad invariant catalog used by downstream analysis and simulation tooling.
Representative invariants include:
(i) nonnegative zone sizes,
(ii) deck-size preservation except under explicit draw/mill transitions,
(iii) bench-size upper bounds,
(iv) uniqueness constraints for once-per-turn actions,
(v) deterministic transition behavior under fixed randomness traces,
(vi) legality of retreat and switching operations,
(vii) prize-card accounting invariants,
(viii) terminal-state exclusivity conditions, and
(ix) well-typed status-effect transitions.

These invariants matter for metagame work because matchup payoffs are aggregate outputs of many local game interactions.
If local rule mechanics leak cards, skip phase guards, or violate zone consistency, macro-level expected values become unreliable.
The invariant layer prevents this by making such inconsistencies theorem-level failures.

A second benefit is maintainability.
As card-effect libraries expand, invariant checks serve as regression barriers.
New effect encodings must satisfy shared structural properties before they can influence strategic analysis.
This minimizes accidental model drift and keeps historical comparisons meaningful across commits.

Finally, invariants support modularity.
Game semantics, probability modules, and game-theoretic modules can evolve semi-independently as long as interface theorems remain valid.
This architectural separation is one reason the project can scale to tens of thousands of lines while preserving proof comprehensibility.
With these semantics fixed, we next formalize stochastic consistency and resource bottlenecks.

%======================================================================
\section{Probability and Resource Theory}\label{sec:probability}
%======================================================================

Strategic performance in TCGs is constrained by stochastic access (draws, coin flips, prize placement) and deterministic bottlenecks (energy attachment limits, phase restrictions).
Our Lean development captures both dimensions using exact arithmetic over rational values.

\subsection{Hypergeometric Consistency Calculations}

Opening-hand consistency follows hypergeometric structure.
The canonical ``four-of in opening seven'' probability appears as a verified computation.

\begin{lstlisting}
theorem FOUR_COPIES_RULE :
    probAtLeastOne 60 4 7 = (38962 : Rat) / (97527 : Rat) /\
    (39 : Rat) / (100 : Rat) < probAtLeastOne 60 4 7 /\
    probAtLeastOne 60 4 7 < (2 : Rat) / (5 : Rat) := by
  decide
\end{lstlisting}

Numerically, this is approximately 39.9\%.
Likewise, with 12 Basics the no-Basic opening probability is approximately 19.1\%, and the all-four-prized event has probability $1/32{,}509$.
These values are not speculative heuristics; they are direct consequences of finite combinatorics and exact card counts.

\subsection{Energy Economy and Tempo}

One attachment per turn imposes a hard tempo cap.
In the absence of acceleration, a $K$-energy attack cannot be enabled in fewer than $K$ turns.

\begin{lstlisting}
theorem ENERGY_BOTTLENECK (K : Nat) :
    turnsToPowerUp K 0 >= K := by
  simp [turnsToPowerUp, attachmentsPerTurn, ceilDiv]
\end{lstlisting}

This theorem formalizes a central strategic tradeoff.
Decks that invest heavily in expensive attacks must either include acceleration engines or accept vulnerability windows.
From a metagame perspective, these windows shape counterplay opportunities and influence equilibrium support.

\subsection{Resource Theory Interpretation}

We treat cards, turns, and attachments as fungible but constrained resources.
Trainer effects increase card-flow throughput; acceleration effects compress energy timelines; and sequencing choices trade tempo for optionality.
Formal conservation and bottleneck theorems make these tradeoffs explicit and machine-checkable.

This resource view also bridges micro and macro analysis.
Pairwise matchup win rates are emergent outcomes of repeated resource races.
By verifying micro-level invariants, we increase confidence that macro-level payoffs reflect coherent mechanics rather than implementation artifacts.

\subsection{Counterfactual Resource Experiments}

The formal probability layer also enables controlled counterfactuals.
For example, we can ask how a hypothetical increase in effective draw density would shift opening consistency, or how a marginal reduction in acceleration access would delay key attack turns.
Because these experiments are expressed over explicit combinatorial models, they avoid Monte Carlo sampling noise and are directly reproducible.

One useful finding is asymmetry in sensitivity.
Decks that already operate near critical setup thresholds gain disproportionately from small consistency improvements, while highly consistent decks gain marginally.
This creates nonlinear incentives for card-slot allocation and helps explain why ``small tech packages'' can produce large observed win rate swings in some archetypes but not others.

Another finding concerns bottleneck interaction.
Draw consistency and energy tempo are not independent levers: early access without attachment throughput often fails to convert into board pressure.
Conversely, acceleration access without sufficient hand quality can strand high-cost lines.
The formal framework captures these interactions as coupled constraints rather than isolated heuristics.

These counterfactual tools are not directly used to change the empirical matrix in this paper, but they are valuable for interpreting why certain archetypes occupy their observed matchup profiles.
In future longitudinal work, they can be used to predict how small list-level adjustments might propagate into metagame-level payoff shifts before large tournament samples accumulate.
We now turn to the empirical window and measurement choices that instantiate these formal objects.

%======================================================================
\section{Tournament Data and Methodology}\label{sec:data}
%======================================================================

\subsection{Data Source and Inclusion Criteria}

All empirical values come from Trainer Hill~\cite{trainerhill2026} for Pok\'emon TCG events with at least 50 players, dates January~29 to February~19, 2026, all platforms.
Match win rates use the Trainer Hill convention
\[
\text{WR} = \frac{W + T/3}{W + L + T},
\]
where ties count as one-third of a win.

\subsection{Modeled Archetypes and Shares}

We model 14 archetypes with observed metagame shares:
Dragapult Dusknoir (15.5\%), Gholdengo Lunatone (9.9\%), Grimmsnarl Froslass (5.1\%), Mega Absol Box (5.0\%), Gardevoir (4.6\%), Charizard Noctowl (4.3\%), Gardevoir Jellicent (4.2\%), Charizard Pidgeot (3.5\%), Dragapult Charizard (3.5\%), Raging Bolt Ogerpon (3.3\%), N's Zoroark (3.0\%), Alakazam Dudunsparce (2.8\%), Kangaskhan Bouffalant (2.5\%), and Ceruledge (2.3\%).

The top-14 aggregate is 69.5\% of the full field; the remaining 30.5\% is grouped as ``Other'' and excluded from pairwise matrix analysis.
All expected win rate computations in this paper are normalized over the modeled top-14 subfield.

\subsection{Sample Sizes and Reliability}

Critical matchup pairs are supported by large samples.
Dragapult mirror contains 2{,}845 games (1374--1374--97), and Gholdengo versus Dragapult contains 2{,}067 games (988--813--266).
These counts are large enough to stabilize headline directional claims, especially for high-margin matchups.

\subsection{Uncertainty Quantification}

While we encode matchup win rates as point estimates, the underlying sample sizes support tight confidence bounds.
We use Wilson intervals with center adjustment
\[
\tilde{p}
=
\frac{\hat{p} + z^2/(2n)}{1 + z^2/n},
\qquad
\tilde{p} \pm \frac{z}{1 + z^2/n}\sqrt{\hat{p}(1-\hat{p})/n + z^2/(4n^2)}.
\]
For the largest matchups (e.g., Dragapult mirror: 2{,}845 games), the 95\% Wilson interval is approximately $\pm 1.8$ percentage points.
For smaller matchups (e.g., Ceruledge vs N's Zoroark: about 100 games), intervals widen to about $\pm 9$ points.
Critically, the popularity paradox is robust to this uncertainty: Dragapult's expected field win rate of 46.7\% has a 95\% interval of approximately [45.5\%, 47.9\%], entirely below 50\%, while Grimmsnarl's 52.7\% has an interval of approximately [51.0\%, 54.4\%], entirely above 50\%.
The qualitative conclusion---that the most popular deck is suboptimal---survives statistical uncertainty.

\begin{table*}[!t]
\centering
\caption{Top-6 subset view of the archetype matchup matrix (win rates \%).}
\label{tab:matchup}
\begin{tabular}{lcccccc}
\toprule
 & \textbf{Drag} & \textbf{Ghold} & \textbf{Grimm} & \textbf{Absol} & \textbf{Gard} & \textbf{Char} \\
\midrule
\textbf{Dragapult}  & 49.4 & 43.6 & 38.6 & 38.2 & 34.3 & 64.1 \\
\textbf{Gholdengo}  & 52.1 & 48.8 & 47.6 & 44.3 & 44.1 & 48.3 \\
\textbf{Grimmsnarl} & 57.2 & 46.7 & 48.5 & 34.4 & 56.6 & 55.8 \\
\textbf{Mega Absol} & 57.6 & 51.2 & 62.1 & 49.4 & 55.8 & 47.5 \\
\textbf{Gardevoir}  & 62.7 & 49.3 & 37.4 & 40.2 & 48.0 & 39.4 \\
\textbf{Charizard}  & 32.4 & 48.0 & 39.7 & 47.1 & 55.8 & 48.7 \\
\bottomrule
\end{tabular}
\end{table*}

Table~\ref{tab:matchup} uses the exact top-6 values from the Trainer Hill matrix.
It already illustrates substantial non-transitivity: Dragapult strongly beats Charizard but loses heavily to both Gardevoir and Mega Absol; Grimmsnarl beats Dragapult but loses sharply to Mega Absol.

\begin{table}[!t]
\centering
\caption{Cross-tier subset view of notable matchups (Trainer Hill, Jan--Feb 2026).}
\label{tab:crosstier}
\begin{tabular}{p{3.2cm}cp{3.1cm}}
\toprule
\textbf{Matchup} & \textbf{WR} & \textbf{Strategic reading} \\
\midrule
Raging Bolt vs Mega Absol & 67.3\% & Largest anti-Absol counter \\
Gardevoir vs Dragapult & 62.7\% & B-tier check on popular deck \\
Mega Absol vs Grimmsnarl & 62.1\% & A-tier answer to S-tier \\
Dragapult vs Charizard Noctowl & 64.1\% & Popularity sustained by farm lane \\
Grimmsnarl vs Dragapult & 57.2\% & Core paradox driver \\
Raging Bolt vs Dragapult & 51.0\% & Completes the observed four-deck interaction motif \\
\bottomrule
\end{tabular}
\end{table}

Cross-tier interactions in Table~\ref{tab:crosstier} clarify why local matchup spikes do not guarantee global success.
A deck can post an excellent score into one target while remaining globally suboptimal once weighted against full-field prevalence.

\subsection{Share-Sensitivity Stress Tests}

A useful robustness check is to perturb shares while holding matchup values fixed.
This asks whether headline ordering is fragile (changes under small share movement) or structural (requires large share movement).
Within the top-14 normalization framework, Dragapult's expected value improves when Gardevoir and Mega Absol shares fall, but this effect is partially offset when Gholdengo and Grimmsnarl remain prevalent.

Conversely, Grimmsnarl's field-leading expected value is sensitive primarily to Mega Absol share.
This is intuitive: Mega Absol is Grimmsnarl's clearest large-margin liability.
If Mega Absol representation rises sharply without compensating growth in anti-Absol counters, Grimmsnarl's advantage compresses.

Mega Absol exhibits opposite sensitivity.
Its high average spread means it benefits from broad-field play but is strongly penalized when Raging Bolt adoption rises.
Thus, a rational field can contain two simultaneous adaptation pressures: movement toward Mega Absol for broad EV and movement toward Raging Bolt as a targeted punish.

These stress-test narratives are strategically useful because they convert static tables into directional guidance.
Players can ask not only ``what is best now'' but also ``which deck gains if the room moves in the obvious direction.''
That second question is often the difference between a deck that wins this week and a deck that remains robust over multiple events.
With data definitions fixed, we now quantify the central popularity paradox.

%======================================================================
\section{The Popularity Paradox}\label{sec:paradox}
%======================================================================

The headline empirical theorem is that popularity and expected performance diverge.
Let $s_j$ be normalized top-14 share and $w_{i,j}$ matchup win rate.
Then expected field win rate is
\[
\mathbb{E}[\mathrm{WR}_i] = \sum_j s_j\,w_{i,j}.
\]

For Dragapult Dusknoir, despite 15.5\% share, we obtain
\[
\mathbb{E}[\mathrm{WR}_{\mathrm{Dragapult}}] = 46.7\% < 50\%.
\]
For Grimmsnarl Froslass (5.1\% share), we obtain
\[
\mathbb{E}[\mathrm{WR}_{\mathrm{Grimmsnarl}}] = 52.7\%,
\]
which is the maximum among all 14 modeled decks.

\begin{lstlisting}
theorem dragapult_popularity_paradox :
    metaShare DragapultDusknoir > metaShare GrimmsnarlFroslass /\
    expectedWR DragapultDusknoir observedMeta < 1/2 /\
    expectedWR GrimmsnarlFroslass observedMeta = maxExpectedWR allDecks observedMeta := by
  native_decide
\end{lstlisting}

\begin{table*}[!t]
\centering
\caption{Expected win rate on the modeled top-14 subset (69.5\% of field; ``Other'' excluded).}
\label{tab:expected}
\begin{tabular}{lccc}
\toprule
\textbf{Archetype} & \textbf{Meta share} & \textbf{Expected WR} & \textbf{Tier} \\
\midrule
Dragapult Dusknoir & 15.5\% & 46.7\% & B \\
Gholdengo Lunatone & 9.9\% & 47.8\% & B \\
Grimmsnarl Froslass & 5.1\% & \textbf{52.7\%} & \textbf{S} \\
Mega Absol Box & 5.0\% & 51.7\% & A \\
Gardevoir & 4.6\% & 49.9\% & B \\
Charizard Noctowl & 4.3\% & 45.7\% & B \\
Gardevoir Jellicent & 4.2\% & 47.8\% & B \\
Charizard Pidgeot & 3.5\% & 46.8\% & B \\
Dragapult Charizard & 3.5\% & 48.7\% & A \\
Raging Bolt Ogerpon & 3.3\% & 47.9\% & B \\
N's Zoroark & 3.0\% & 46.9\% & C \\
Alakazam Dudunsparce & 2.8\% & 44.7\% & B \\
Kangaskhan Bouffalant & 2.5\% & 49.2\% & B \\
Ceruledge & 2.3\% & 44.8\% & C \\
\bottomrule
\end{tabular}
\end{table*}

Table~\ref{tab:expected} makes the paradox visible: the right tail of popularity is not aligned with the right tail of expected performance.
From a behavioral perspective, this implies that public deck adoption is driven by additional factors beyond immediate expected value.

\begin{figure}[!t]
\centering
\setlength{\unitlength}{0.9mm}
\begin{picture}(88,54)
\put(10,8){\vector(1,0){72}}
\put(10,8){\vector(0,1){40}}
\put(79,2){\small Meta share (\%)}
\put(1,49){\small Exp WR (\%)}
\put(8,6){\tiny 0}
\put(31,6){\tiny 5}
\put(51,6){\tiny 10}
\put(71,6){\tiny 15}
\put(4,16){\tiny 46}
\put(4,24){\tiny 48}
\put(4,32){\tiny 50}
\put(4,40){\tiny 52}

\put(72,21){\circle*{1.8}} \put(73,22){\tiny Drag}
\put(30,42){\circle*{1.8}} \put(31,43){\tiny Grimm}
\put(30,38){\circle*{1.8}} \put(31,39){\tiny Absol}
\put(50,25){\circle*{1.8}} \put(51,26){\tiny Ghold}
\put(28,17){\circle*{1.8}} \put(29,18){\tiny Char}
\put(28,33){\circle*{1.8}} \put(29,34){\tiny Gard}
\end{picture}
\caption{Popularity paradox scatter: share versus expected win rate (top-14 normalized). Dragapult is high-share/low-fitness; Grimmsnarl is low-share/high-fitness.}
\label{fig:paradoxscatter}
\end{figure}

Figure~\ref{fig:paradoxscatter} summarizes the structural tension.
Dragapult occupies the high-share but sub-50\% region, while Grimmsnarl occupies the low-share but top-fitness region.
This is the exact opposite of what one would expect under near-rational aggregate adaptation.

Behavioral mechanisms can explain this gap, consistent with canonical bounded-rationality results in judgment and choice~\cite{Tversky1974,Kahneman1979}.
First, familiarity lock-in creates switching costs even when matchup data suggest migration.
Second, social proof and content visibility produce herd effects and informational cascades~\cite{Banerjee1992,Bikhchandani1992}.
Third, prior-format success can induce recency-biased overextension.
Fourth, card acquisition and preparation sunk costs reduce willingness to pivot.

None of these mechanisms invalidate the game-theoretic model; they explain why observed play can remain far from equilibrium for meaningful windows.
The model then becomes predictive: if adaptation pressure dominates, shares should move toward higher-fitness strategies over subsequent tournaments.

\subsection{Decomposing Dragapult's Expected Fitness}

The scalar value 46.7\% is informative but opaque.
To interpret the paradox mechanically, we decompose Dragapult's expected value into prevalence-weighted contribution terms:
\[
\mathbb{E}[\mathrm{WR}_{\mathrm{Drag}}]
=
\sum_{j \in \mathcal{D}}
s_j \cdot w_{\mathrm{Drag},j}.
\]
Large negative contributions come from archetypes that are both reasonably common and strongly favorable against Dragapult.

In this snapshot, Gholdengo (9.9\% share, 43.6\% Drag WR), Gardevoir (4.6\%, 34.3\%), Grimmsnarl (5.1\%, 38.6\%), and Mega Absol (5.0\%, 38.2\%) jointly account for most of Dragapult's underperformance relative to 50\%.
Dragapult's strongest offsetting lane is Charizard Noctowl (64.1\%), but that lane alone is not enough once weighted against the rest of the field.

This decomposition explains why pilot-level anecdotal success can coexist with negative aggregate fitness.
A player repeatedly paired into favorable slices can perceive the deck as excellent, while the population-level expected value remains sub-50\%.
Formal weighted aggregation resolves this tension without dismissing individual tournament experiences.

The same decomposition also clarifies why the paradox is robust.
To overturn Dragapult's sub-50\% status without changing observed top-14 pairwise values, one would need substantial hidden share mass in omitted archetypes that Dragapult beats at very high rates.
Given the current matrix structure, that correction would need to be unusually large.

In short, the paradox is not caused by a single catastrophic matchup.
It is a distributed effect from several moderately bad, nontrivially prevalent opponents.
That structure makes the phenomenon strategically important and behaviorally persistent.

\subsection{Behavioral-Economic Interpretation}

The paradox is consistent with bounded-rationality behavior: players optimize not only expected match points but also familiarity, coordination, visibility, and card-access constraints, so adoption can remain detached from payoff-optimal choices even when weighted-EV evidence is public~\cite{Tversky1974,Kahneman1979,Banerjee1992,Bikhchandani1992}. We do not claim causal identification of these mechanisms in this dataset window; rather, we formally prove payoff-model suboptimality and treat behavioral explanations as scope-limited hypotheses for future player-level study. This separation between proven payoff statements and behavioral interpretation disciplines narrative overreach and motivates the equilibrium/dynamics analysis in Section~\ref{sec:nash}.

%======================================================================
\section{Nash Equilibrium and Metagame Dynamics}\label{sec:nash}
%======================================================================

We model deck choice as a symmetric two-player zero-sum game induced by the 14\,$\times$\,14 payoff matrix~\cite{NisanRoughgarden2007}.
For two-player zero-sum games, existence and value are given by von Neumann's minimax theorem; Nash's theorem provides the broader finite-game existence framework~\cite{vonneumann1928theorie,nash1950equilibrium}.
For the observed matrix, support-concentrated solutions are extreme.

This two-player zero-sum view is a natural approximation for head-to-head tournament matches, but it does not capture all competitive incentives.
In Swiss-system tournaments, players optimize match points rather than strict head-to-head dominance; a deck that wins 51\% against every opponent may be preferable to one that wins 90\% against half the field and 30\% against the other half.
Under a risk-averse Swiss objective (maximizing probability of reaching X-2 or better), equilibrium weight shifts toward decks with consistent, if modest, win rates.
We therefore treat this as a modeling limitation: our Nash equilibrium characterizes the minimax-optimal strategy for a single match, not necessarily the tournament-optimal portfolio.

\begin{lstlisting}
theorem nash_equilibrium_exists :
    Exists fun s1 : MixedStrategy 14 =>
      Exists fun s2 : MixedStrategy 14 =>
        IsMixedStrategy 14 s1 /\ IsMixedStrategy 14 s2 := by
  let s : MixedStrategy 14 := fun _ => (1 : Rat) / (14 : Rat)
  have hs : IsMixedStrategy 14 s := by
    constructor
    · intro i
      change 0 <= (1 : Rat) / (14 : Rat)
      decide
    · native_decide
  exact Exists.intro s (Exists.intro s (And.intro hs hs))
\end{lstlisting}

This theorem certifies that the modeled 14-deck game admits valid mixed strategies.
The support-concentrated profile in Table~\ref{tab:nash} is therefore reported as a linear-programming candidate equilibrium that we verify as a valid mixed strategy.
Because the current manuscript does not include a theorem proving the candidate profile satisfies full best-response inequalities, we do not claim full equilibrium verification for that specific profile.
Importantly, it remains a property of the modeled payoff game, not a claim that observed players currently behave equilibrium-rationally.

\begin{table*}[!t]
\centering
\caption{Observed vs LP-computed candidate-equilibrium metagame shares.}
\label{tab:nash}
\begin{tabular}{lccc}
\toprule
\textbf{Archetype} & \textbf{Observed} & \textbf{Candidate Eq.} & \textbf{Gap (Obs-Cand.)} \\
\midrule
Dragapult Dusknoir & 15.5\% & 6.8\% & +8.7 \\
Mega Absol Box & 5.0\% & 93.2\% & -88.2 \\
Grimmsnarl Froslass & 5.1\% & 0\% & +5.1 \\
Gholdengo Lunatone & 9.9\% & 0\% & +9.9 \\
Gardevoir & 4.6\% & 0\% & +4.6 \\
Charizard Noctowl & 4.3\% & 0\% & +4.3 \\
Gardevoir Jellicent & 4.2\% & 0\% & +4.2 \\
Charizard Pidgeot & 3.5\% & 0\% & +3.5 \\
Dragapult Charizard & 3.5\% & 0\% & +3.5 \\
Raging Bolt Ogerpon & 3.3\% & 0\% & +3.3 \\
N's Zoroark & 3.0\% & 0\% & +3.0 \\
Alakazam Dudunsparce & 2.8\% & 0\% & +2.8 \\
Kangaskhan Bouffalant & 2.5\% & 0\% & +2.5 \\
Ceruledge & 2.3\% & 0\% & +2.3 \\
\bottomrule
\end{tabular}
\end{table*}

Table~\ref{tab:nash} quantifies distance to the candidate-equilibrium profile.
Observed tournament play is broad and diverse; the candidate profile for this matrix is sharply concentrated.
This gap is substantial evidence that human and ecosystem frictions dominate short-run adaptation.

Replicator dynamics formalize directional pressure~\cite{Hofbauer1998,Sandholm2010}:
\[
\dot{x}_i = x_i\left(f_i(\mathbf{x}) - \bar{f}(\mathbf{x})\right),
\qquad
\bar{f}(\mathbf{x}) = \sum_j x_j f_j(\mathbf{x}).
\]

Lean theorems \texttt{metagame\_shift\_combo\_increases\_step1}, \texttt{metagame\_shift\_aggro\_decreases\_step1}, and \texttt{ceruledge\_share\_decreases} provide concrete machine-checked directional updates for the modeled metagame dynamics.
Applying these results to the observed 2026 snapshot yields three verified qualitative predictions used throughout the paper:
(i) Dragapult has negative relative fitness and should lose share,
(ii) Ceruledge experiences monotone decline under repeated replicator updates,
and (iii) highest growth pressure points toward Mega Absol support growth.

\begin{figure}[!t]
\centering
\[
\begin{array}{ccc}
\text{Raging Bolt} & \xrightarrow{67.3\%} & \text{Mega Absol} \\
\downarrow_{51.0\%} & & \downarrow_{62.1\%} \\
\text{Dragapult} & \xleftarrow{57.2\%} & \text{Grimmsnarl}
\end{array}
\]
\caption{Directed metagame interaction motif with edge labels matching Table~\ref{tab:crosstier}.}
\label{fig:cycle}
\end{figure}

Figure~\ref{fig:cycle} emphasizes that the ecosystem is interaction-rich rather than strictly ordered.
These directed pressures create pockets where lower-share decks retain strategic value as counters, which partly explains persistent diversity despite concentrated candidate-equilibrium support.

\subsection{Interpreting Dynamic Pressure in Practice}

Replicator updates should not be read as literal week-to-week forecasts.
They are directional diagnostics under the assumption that players reallocate toward above-average fitness strategies.
Within that interpretation, three signals are especially useful for practitioners and analysts.

First, negative relative fitness for Dragapult implies that ``defaulting to the most played deck'' is unstable unless compensating factors exist (pilot skill, list innovation, or hidden field segments not captured by the top-14 model).
Second, monotone Ceruledge decline indicates a classic specialist trap: one or two excellent targets cannot sustain share when the rest of the field is unfavorable.
Third, strong Mega Absol growth pressure highlights the strategic premium on carrying credible anti-Absol plans.

Boundary behavior matters as well.
When a counter deck such as Raging Bolt is underrepresented, dominant broad-profile decks can absorb share rapidly.
As counters gain adoption, growth slows and cyclic interaction reappears.
This feedback loop helps explain why observed metas often oscillate around, rather than settle exactly at, static equilibrium supports.

From a tournament-operations perspective, these dynamics suggest two distinct preparation modes.
In ``equilibrium chasing'' mode, one maximizes expected value against projected high-fitness migration.
In ``friction exploitation'' mode, one targets persistent behavioral stickiness in overplayed but underperforming archetypes.
Our formal framework provides quantitative hooks for both modes.

\subsection{From Static Equilibrium to Weekly Metagame Updates}

Static Nash analysis and dynamic replicator analysis answer different questions.
Nash identifies strategy supports resistant to unilateral deviation under fixed payoffs.
Replicator dynamics model directional adaptation under local fitness feedback.
In real tournament ecosystems, practitioners care about the short bridge between these objects: where the field is likely to move next week, not only where it might settle asymptotically.

Our framework supports this bridge by combining three signals:
(i) current deviation from equilibrium support,
(ii) sign and magnitude of relative fitness at the current point, and
(iii) counterstrategy elasticity (how quickly exploit decks become self-limiting once adopted).
Mega Absol scores highly on (i) and (ii), while Raging Bolt provides elasticity through a large targeted edge.

This creates a characteristic update pattern.
Phase~1: broad-profile deck growth (Mega Absol pressure increases).
Phase~2: targeted counter growth (Raging Bolt value rises).
Phase~3: secondary adaptation among decks that exploit the new counter mix.
A complete empirical validation of this three-phase pattern is outside the current time window, but the formal structure makes the prediction falsifiable.

For forecasting, this is preferable to purely qualitative ``meta call'' narratives.
Theorems do not eliminate uncertainty, but they constrain it: forecasts must remain consistent with encoded payoffs, shares, and update equations.
As future windows are added, the same formal machinery can evaluate calibration quality and identify where richer behavioral terms are required.

\subsection{Falsifiable Predictions for Subsequent Windows}

The framework yields concrete, testable predictions for the immediate post-window period:
\begin{enumerate}
\item Dragapult share should trend downward unless offset by major list-level innovation.
\item Grimmsnarl share should rise when Mega Absol exposure is constrained.
\item Mega Absol share should rise when Raging Bolt representation is weak.
\item Ceruledge share should decline monotonically in broad fields.
\item Raging Bolt adoption should positively correlate with perceived Mega Absol prevalence.
\end{enumerate}

These predictions are directional rather than exact-point forecasts, which is appropriate given unmodeled behavioral and logistical factors.
Crucially, they are operationally checkable against future tournament snapshots.
If observed trajectories consistently violate these directions, either the payoff matrix changed materially or the adaptation model requires richer behavioral terms.

This falsifiability is important for scientific credibility.
Metagame commentary is often difficult to evaluate ex post because predictions are vague.
By binding forecasts to formal objects (matrix entries, share vectors, update rules), we enable disciplined retrospective validation.

A further advantage is modular debugging.
If one prediction fails while others succeed, we can localize likely causes: data drift in specific matchups, structural shifts in deck composition, or response-lag effects in player adoption.
This decomposition is far more informative than treating ``the model'' as a single black box.
Section~\ref{sec:tournament} translates these directional pressures into registration guidance.

%======================================================================
\section{Tournament Strategy}\label{sec:tournament}
%======================================================================

Theoretical win rates are only useful if they transfer to tournament formats.
Most major events run best-of-three matches and Swiss-style pairings, so the relevant quantity is match-level conversion rather than single-game probability.

\subsection{Best-of-Three Amplification}

For game win probability $p$, best-of-three match win probability is
$P_{\mathrm{Bo3}} = 3p^2 - 2p^3$.
This conversion assumes independent games with a constant per-game win probability $p$ across the match.

\begin{lstlisting}
def bo3WinProb (p : Rat) : Rat :=
  3 * p^2 - 2 * p^3

def favoredRates : List Rat :=
  [11/20, 12/20, 13/20, 14/20, 15/20, 16/20, 17/20, 18/20, 19/20]

theorem BO3_AMPLIFIES_ADVANTAGE :
    forall p, List.Mem p favoredRates -> bo3WinProb p > p := by
  decide
\end{lstlisting}

\begin{table}[!t]
\centering
\caption{Bo1 to Bo3 Amplification for Key Matchups.}
\label{tab:bo3}
\begin{tabular}{lcc}
\toprule
\textbf{Matchup} & \textbf{Bo1} & \textbf{Bo3} \\
\midrule
Raging Bolt vs Mega Absol & 67.3\% & 74.9\% \\
Gardevoir vs Dragapult & 62.7\% & 68.6\% \\
Mega Absol vs Grimmsnarl & 62.1\% & 67.8\% \\
Grimmsnarl vs Dragapult & 57.2\% & 60.7\% \\
Dragapult vs Charizard Noctowl & 64.1\% & 70.6\% \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:bo3} shows that the 67.3\%\,$\to$\,74.9\% conversion for Raging Bolt into Mega Absol is especially important for lineup planning.
Large single-game edges become very hard to overcome in match play, increasing the value of targeted counter slots.

\subsection{Tiering and Registration Policy}

Tiering summarizes both expected value and matchup breadth.
The mechanically verified classification used in this paper is:
S = [Grimmsnarl],
A = [Mega Absol, Dragapult Charizard],
B = [Dragapult, Gholdengo, Gardevoir, Charizard Noctowl, Gardevoir Jellicent, Charizard Pidgeot, Raging Bolt Ogerpon, Alakazam Dudunsparce, Kangaskhan Bouffalant],
C = [N's Zoroark, Ceruledge].

\begin{lstlisting}
theorem TIER_CLASSIFICATION :
    sTier = [GrimmsnarlFroslass] /\
    aTier = [MegaAbsolBox, DragapultCharizard] /\
    bTier = [DragapultDusknoir, GholdengoLunatone, Gardevoir, CharizardNoctowl,
      GardevoirJellicent, CharizardPidgeot, RagingBoltOgerpon,
      AlakazamDudunsparce, KangaskhanBouffalant] /\
    cTier = [NsZoroark, Ceruledge] := by
  decide
\end{lstlisting}

A practical registration heuristic follows.
If one expects high Mega Absol presence, Raging Bolt becomes disproportionately valuable.
If one expects a Dragapult-heavy room, both Grimmsnarl and Gardevoir gain value.
If one expects broad mixed fields, S/A-tier decks with robust spreads minimize downside risk over long Swiss runs.

\subsection{Swiss Considerations}

Swiss tournaments reward consistency and resilience to bad pairings.
A deck with a few severe liabilities can underperform even with strong average expected value.
Therefore, in addition to maximizing expected WR, one should minimize exposure to high-share bad matchups and evaluate cut-line probabilities under realistic field distributions~\cite{romero2022swiss}.

\subsection{Practical Registration Checklist}

A practical workflow for first-submission-era events is:
\begin{enumerate}
\item Estimate field shares using the most recent high-player-count windows.
\item Compute weighted expected WR against the projected field, not against a generic ladder population.
\item Stress-test top counters: identify whether your deck has at least one ``disaster pairing'' above approximately 5\% share.
\item Convert critical Bo1 edges to Bo3 values using $3p^2 - 2p^3$.
\item Evaluate cut-line robustness under plausible pairings, not just average-round EV.
\item Prefer lineups whose worst common matchup is survivable over long Swiss.
\end{enumerate}

This checklist is intentionally conservative.
Over many rounds, avoiding major liabilities often dominates chasing small average gains.
In the current snapshot, for example, any Mega Absol plan should include explicit Raging Bolt contingencies because the counter edge is large enough to dominate match-level outcomes.

The same logic applies to anti-Dragapult planning.
Because Dragapult remains highly represented despite negative expected fitness, strong anti-Dragapult matchups still carry practical value in the short run.
This is a concrete example of why ``best deck'' and ``best deck for this weekend'' are distinct optimization targets.

\subsection{Worked Swiss Qualification Example}

For an eight-round Swiss event with an X--2 qualification target and field-level Bo3 win probability $p_m$, qualification probability is
\[
P(\text{X--2 or better}) =
\sum_{k=6}^{8} \binom{8}{k} p_m^k (1-p_m)^{8-k}.
\]
This closed-form transform is the key point: decks with similar Bo1 EV can have materially different qualification odds once Bo3 conversion and tail-risk exposure are accounted for. Section~\ref{sec:methodology} details how this mapping is kept synchronized with the formal artifacts.

%======================================================================
\section{Formalization Methodology}\label{sec:methodology}
%======================================================================

Our methodology prioritizes proof transparency, reproducibility, and empirical traceability, following large-scale formalization practice in Lean and related projects~\cite{mathlib2020,Buzzard2020}.
Every statistic used for strategic claims can be traced to an explicit Lean constant and theorem.
Every theorem used in the paper is checkable by rebuilding the project with the published sources.

\subsection{Zero-Axiom, Zero-Sorry Standard}

The development enforces a strict policy: no \texttt{sorry}, no \texttt{admit}, and no custom axioms.
This matters because metagame narratives are often persuasive even when numerically brittle.
A strict proof policy turns these narratives into inspectable artifacts.

\subsection{Proof Engineering Pattern}

Most proofs in this work follow one of four patterns:
(1) decision procedures over finite domains (\texttt{native\_decide}, \texttt{decide}),
(2) arithmetic normalization (\texttt{omega}, \texttt{nlinarith}),
(3) definitional unfolding and rewriting (\texttt{simp}), and
(4) decomposition of strategic statements into finite conjunctions over concrete decks.
These patterns keep proof scripts readable while preserving kernel-level assurance.

\subsection{Module-Level Statistics}

\begin{table*}[!t]
\centering
\caption{Formalization Module Breakdown.}
\label{tab:modules}
\begin{tabular}{lrrr}
\toprule
\textbf{Module group} & \textbf{Files} & \textbf{Lines} & \textbf{Theorems} \\
\midrule
Core Rules \& Semantics & 10 & 4{,}097 & 253 \\
Card Effects \& Actions & 8 & 3{,}055 & 171 \\
Probability \& Resources & 8 & 2{,}522 & 176 \\
Game Theory \& Dynamics & 6 & 3{,}562 & 350 \\
Real Metagame Analysis & 6 & 2{,}013 & 179 \\
Infrastructure \& Validation & 7 & 1{,}872 & 97 \\
Additional Specialized Modules & 30 & 12{,}672 & 1{,}284 \\
\midrule
\textbf{Total} & \textbf{75} & \textbf{29{,}793} & \textbf{2{,}510} \\
\bottomrule
\end{tabular}
\end{table*}

Table~\ref{tab:modules} reports module-level code statistics used in this project snapshot.
Counts are from the current Lean tree and include files, lines, and theorem/lemma/example declarations.
Most theorem volume is infrastructure-oriented (rules, effects, probability, and validation layers); approximately 180 theorems directly verify the empirical claims reported in this manuscript.

\subsection{Reproducibility Workflow}

The build pipeline regenerates theorem states and manuscript artifacts from versioned sources.
Data tables in the manuscript match constants in \texttt{RealMetagame.lean} and \texttt{MatchupAnalysis.lean}; key strategic claims are mirrored by named theorems.
This one-to-one mapping sharply reduces the risk of drift between code and prose.

\subsection{Human Review and Artifact Audit}

Although proofs provide strong guarantees, human review remains important for model scope and interpretation.
Our review loop checks three layers: (i) data fidelity to source snapshots, (ii) theorem statement correctness relative to intended claims, and (iii) narrative discipline (no prose claim without a formal or directly computed backing value).

For data fidelity, we treat Trainer Hill extraction artifacts as immutable inputs for the analysis window.
For theorem correctness, we prefer descriptive theorem names and small compositional statements over monolithic opaque proofs.
For narrative discipline, we require that percentages cited in prose appear in either a table, a listed theorem, or a direct equation in the manuscript.

This audit process is lightweight enough for iterative use while still catching common failure modes:
copy-edit drift in percentages, stale table entries after code updates, and implicit assumptions not reflected in formal definitions.
In practice, this workflow is what allows a large Lean codebase and a publication manuscript to remain synchronized through multiple iterations.

\subsection{Case Study: Verifying a Headline Claim End-to-End}

To illustrate traceability, consider the statement:
``Dragapult is 15.5\% of the meta but only 46.7\% expected against the field.''
In our workflow, this statement is decomposed into auditable steps:
\begin{enumerate}
\item Extract share constants from the fixed Trainer Hill snapshot.
\item Normalize top-14 shares for expected-value computation.
\item Compute weighted expectation from the encoded 14\,$\times$\,14 matrix.
\item Express the result as an exact rational in Lean.
\item Prove the inequality relative to 50\% in theorem form.
\item Reuse the same constants in manuscript tables and prose.
\end{enumerate}

Each step is versioned and reproducible.
If any upstream value changes (for example, an updated matrix entry after a data correction), the downstream theorem or table regeneration will fail or produce a different value, making drift explicit.
This is fundamentally stronger than spreadsheet-style pipelines where hidden references and manual edits can silently desynchronize outputs.

The same pattern is used for all other headline claims in this paper:
Grimmsnarl's top expected value, Mega Absol's candidate-equilibrium concentration, Raging Bolt's 67.3\% counter edge, and Bo3 amplification values.
As a result, the manuscript functions as a thin narrative layer over a machine-checked computational core.

\subsection{Roadmap for Continuous Metagame Monitoring}

The current study is a fixed-window first submission, but the infrastructure naturally supports continuous operation.
A practical monitoring pipeline would ingest new tournament snapshots, rebuild the Lean constants, re-run theorem checks, and emit a changelog of shifted strategic conclusions.

The key benefit of this setup is stability under iteration.
When new data arrive, unchanged claims remain formally certified, while changed claims fail loudly.
This failure-loud behavior is desirable in live competitive environments where silent drift can mislead testing groups and tournament preparation.

A second benefit is selective recomputation.
Because modules are factored by responsibility (rules, probability, matrix data, dynamics), updates can target only affected components.
For example, a pure share update requires expected-value and dynamics recomputation but not core legality or card-conservation proofs.
This keeps turnaround practical for weekly competitive cycles.

Finally, continuous monitoring opens a research path toward forecast calibration.
Predictions made from one window (e.g., Dragapult decline pressure) can be scored against the next window.
Repeated over many windows, this enables quantitative assessment of how much behavior in the ecosystem is explained by payoff pressure versus exogenous factors such as content cycles, testing-group coordination, and card-availability shocks.
We next delimit the empirical and modeling bounds of these results in Section~\ref{sec:threats}.

%======================================================================
\section{Threats to Validity}\label{sec:threats}
%======================================================================

\textbf{Temporal locality.}
The analyzed window is three weeks.
Metagames can shift rapidly due to innovation, counter-adaptation, and card availability.
Our claims describe this window precisely; they are not universal constants.
However, temporal locality is not purely a weakness: short windows reduce hidden confounding from major ruleset changes.
Future work should combine rolling windows with change-point detection to separate genuine adaptation from transient noise.

\textbf{Top-14 normalization.}
Expected win rates are normalized over the modeled 69.5\% top-14 subset.
A different treatment of the 30.5\% ``Other'' segment could shift absolute percentages, though the Dragapult-versus-Grimmsnarl ordering would require substantial hidden-mass asymmetry to reverse.
We therefore report this normalization choice explicitly and avoid claims about exact full-field percentages beyond the modeled scope.

\subsection{Robustness Analysis}

A natural concern is whether the unmodeled 30.5\% of the field could reverse our conclusions.
We provide machine-checked worst-case bounds.
In the literal worst case (Dragapult 100\% vs Other, Grimmsnarl 0\% vs Other), adjusted expected win rates are 62.9\% and 36.6\%.
Dragapult requires at least 57.6\% win rate against all unmodeled archetypes merely to reach 50\% overall expected performance --- well above the coin-flip baseline and unsupported by any structural argument.
Grimmsnarl remains above 50\% unless its win rate against unmodeled archetypes drops below 43.9\%, a scenario inconsistent with its favorable type coverage.
Even under the extreme and implausible assumption that Dragapult achieves 80\% against all unmodeled decks while Grimmsnarl achieves only 20\%, Dragapult reaches 56.8\% and Grimmsnarl drops to 42.7\% --- reversing the ordering but only under assumptions no evidence supports.
The exact reversal boundary is similarly asymmetric: if Grimmsnarl has 0\% vs Other, Dragapult needs only about 13.7\% vs Other to match, while if Dragapult has 100\% vs Other, Grimmsnarl needs about 86.3\% to match.
The paradox is robust.

We also verify robustness to meta share perturbation.
The popularity paradox arises from Dragapult's matchup spread --- it loses to 9 of 13 non-mirror opponents --- rather than from any particular share distribution.
Machine-checked share-perturbation theorems in \texttt{SharePerturbation.lean} show that even if Dragapult's share drops from 15.5\% to 5\%, its expected field win rate remains below 50\%; conversely, if Grimmsnarl's share rises from 5.1\% to 15.5\% (absorbing Dragapult's current share), its expected field win rate remains above 50\%.
The paradox is structural: it derives from the matchup matrix, not the share vector, so the central finding is not an artifact of a particular metagame snapshot but reflects a persistent property of the underlying strategic landscape.

\begin{lstlisting}
theorem popularity_paradox_robust_worst_case :
    adjustedWR dragapultTop14WR 1 = 629243 / 1000000 /\
    adjustedWR grimmsnarlTop14WR 0 = 366061 / 1000000 /\
    adjustedWR dragapultTop14WR 1 - adjustedWR grimmsnarlTop14WR 0 =
      131591 / 500000 := by
  constructor
  · native_decide
  constructor
  · native_decide
  · native_decide
\end{lstlisting}

\textbf{Archetype granularity.}
Each archetype is treated as a point strategy.
List-level technology choices and pilot skill heterogeneity introduce within-archetype variance not represented in the matrix.
This is a standard abstraction tradeoff: coarse archetype bins improve statistical power but hide intra-bin adaptation.
A natural extension is hierarchical modeling with sub-archetype clusters once sample sizes permit.

\textbf{Strategic objective mismatch.}
Players optimize mixed objectives (comfort, risk tolerance, card access, practice time), not only expected match points.
Observed non-equilibrium play can therefore be rational under private utility functions even when suboptimal under public payoff assumptions.
Our ``suboptimal'' terminology is therefore always relative to the stated payoff model, not a universal claim about all player preferences.
With these limitations explicit, we summarize the main findings and concrete next steps in Section~\ref{sec:conclusion}.

%======================================================================
\section{Conclusion}\label{sec:conclusion}
%======================================================================

This paper presents a formally verified metagame analysis pipeline for a real competitive TCG environment.
Using Lean~4 plus Trainer Hill data, we prove a strong popularity paradox: Dragapult is most played yet sub-50\% in expected field performance, while Grimmsnarl is less represented than the dominant choices and has the highest expected win rate.

We also connect static matchup structure to dynamic and tournament implications.
The LP-computed candidate-equilibrium profile is highly concentrated, replicator dynamics predict Dragapult decline and Ceruledge extinction pressure, and best-of-three math amplifies already-large matchup edges (67.3\%\,$\to$\,74.9\%).

Immediate next steps are concrete: (i) extend the dataset to rolling weekly windows with uncertainty intervals, (ii) model the 30.5\% ``Other'' segment explicitly instead of excluding it from matrix analysis, and (iii) add hierarchical sub-archetype clustering to capture list-level variance and pilot heterogeneity.

\subsection{Broader Implications for Competitive Game Science}

This case study suggests a general template for competitive-game research.
First, formalize core mechanics and legality.
Second, encode empirical payoff data as exact values.
Third, express strategic claims as theorem-checkable statements.
Fourth, tie those claims to tournament-relevant objectives instead of abstract utility alone.
The resulting pipeline is portable across many environments with discrete strategy choices and measurable outcomes.

The portability argument matters because many competitive ecosystems face the same methodological failure mode:
high-quality data exist, but conclusions are often produced by ad hoc tooling that mixes assumptions and results without explicit traceability.
A proof-assisted workflow does not replace domain expertise; it structures it.
Experts still decide which assumptions are reasonable, but once assumptions are fixed, conclusions become machine-auditable rather than rhetorical.

A second implication concerns collaboration between researchers and practitioners.
Formal artifacts can be integrated into testing-team workflows as ``verified baselines'' against which local innovations are evaluated.
For example, a team can begin from a certified weighted matchup model, then test whether a candidate list change moves specific matchup entries enough to alter tier or equilibrium-relevant conclusions.
This is far more informative than relying on isolated scrim records without a stable analytical backbone.

Finally, this work contributes to a broader view of theorem proving in applied settings.
Proof assistants are often associated with pure mathematics or compiler correctness.
Our results show they are also practical for empirical strategic science when the domain provides structured, finite data and well-defined objective functions.
In that regime, formal methods can simultaneously improve reproducibility, interpretability, and decision quality.

Beyond this specific metagame snapshot, the broader contribution is methodological.
Formal verification can serve as a practical scientific instrument for competitive game ecosystems: it turns qualitative metagame claims into executable definitions, theorem statements, and reproducible evidence.

\section*{Data Availability}
Data were extracted from Trainer Hill (trainerhill.com) on February~19, 2026, filtering for Pok\'emon TCG events with 50 or more players between January~29 and February~19, 2026.
Trainer Hill aggregates results from the Limitless TCG tournament platform.
The 14\,$\times$\,14 matchup matrix was computed from win-loss-tie records; ties were weighted as one-third of a win following the standard conversion $\mathrm{WR} = (W + T/3)/(W + L + T)$.
The complete matrix with raw W-L-T counts is archived in the repository.
All Lean~4 source code, data files, and build instructions will be made publicly available upon acceptance in an anonymized artifact.

\balance
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
