\documentclass[journal]{IEEEtran}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{cite}
\usepackage{url}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{balance}
\usepackage{needspace}

\lstdefinelanguage{Lean}{
  morekeywords={theorem,def,structure,inductive,where,match,with,if,then,else,let,in,fun,forall,exists,by,have,show,sorry,exact,simp,omega,decide,native_decide,intro,apply,rfl,instance,class,abbrev,noncomputable,example,lemma,Prop,Type,Nat,Fin,List,Bool,true,false,And,Or,Not},
  sensitive=true,
  morecomment=[l]{--},
  morestring=[b]"
}

\lstset{
  language=Lean,
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue!55!black}\bfseries,
  commentstyle=\color{green!40!black}\itshape,
  stringstyle=\color{orange!50!black},
  frame=single,
  breaklines=true,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny\color{gray},
  xleftmargin=1.5em,
  aboveskip=0.5em,
  belowskip=0.5em,
  literate={·}{{$\cdot$}}1 {←}{{$\leftarrow$}}1 {→}{{$\rightarrow$}}1 {∀}{{$\forall$}}1 {∃}{{$\exists$}}1 {≤}{{$\leq$}}1 {≥}{{$\geq$}}1 {≠}{{$\neq$}}1 {⟨}{{$\langle$}}1 {⟩}{{$\rangle$}}1 {↔}{{$\leftrightarrow$}}1 {∧}{{$\land$}}1 {∨}{{$\lor$}}1 {¬}{{$\lnot$}}1 {λ}{{$\lambda$}}1 {⊢}{{$\vdash$}}1 {∈}{{$\in$}}1
}

\let\origsection\section
\renewcommand{\section}{\needspace{4\baselineskip}\origsection}
\let\origsubsection\subsection
\renewcommand{\subsection}{\needspace{3\baselineskip}\origsubsection}
\clubpenalty=10000
\widowpenalty=10000
\displaywidowpenalty=10000

\begin{document}

\title{From Rules to Nash Equilibria: Formally Verified\\Game-Theoretic Analysis of a Competitive\\Trading Card Game}

\author{Author names withheld for review}

\maketitle

\begin{abstract}
We present a formally verified analysis of competitive Pok\'emon Trading Card Game metagame dynamics using Lean~4 and real tournament data.
Our formal development spans approximately 30{,}000~lines across 75~Lean files and proves over 2{,}500 theorems without \texttt{sorry}, \texttt{admit}, or custom axioms.
Our strategic model covers the top 14 archetypes (69.5\% of the field); the remaining 30.5\% is excluded from the game-theoretic model but bounded in robustness analysis.
We analyze Trainer Hill tournament data from January--February 2026 (50+ player events), covering a complete 14\,$\times$\,14 matchup matrix.
The central result is a verified popularity paradox: Dragapult Dusknoir is the most played deck (15.5\%) but has only 46.7\% expected win rate, while Grimmsnarl Froslass (5.1\% share) achieves 52.7\%.
We report a machine-checked Nash equilibrium with six-deck support per player; best-response conditions are verified via \texttt{native\_decide} for all 14 pure strategies, and Dragapult receives 0\% equilibrium weight.
On representative 4-deck and 5-deck subgames of the observed metagame, replicator dynamics predict Dragapult decline and Ceruledge extinction pressure.
These results demonstrate that formal methods can produce empirically grounded and strategically actionable conclusions in competitive game ecosystems.
\end{abstract}

\begin{IEEEkeywords}
Formal verification, game theory, trading card games, Nash equilibrium, theorem proving, metagame analysis, replicator dynamics, Lean~4
\end{IEEEkeywords}

%======================================================================
\section{Introduction}
%======================================================================

Tournament outcomes in competitive trading card games (TCGs) are often shaped before round one begins.
Players must make two coupled decisions: how to play each game state and which deck to register.
The first decision is local and tactical; the second is global and game-theoretic.
The pre-tournament deck-selection problem is naturally modeled as a strategic game where payoffs derive from matchup win rates and the population distribution of opponents.

The Pok\'emon TCG is especially suitable for this analysis.
It has a large organized-play ecosystem, clearly defined public rules, and a metagame that evolves quickly enough to produce measurable strategic cycles.
At the same time, the domain is difficult for informal reasoning: hidden information, stochastic effects, and nonlinear tournament incentives make intuition unreliable even for experienced players.

Formal methods offer an attractive remedy.
By encoding game semantics in Lean~4~\cite{moura2021lean} and proving strategic statements directly over exact data representations, we separate factual claims from narrative claims.
A statement in the paper is either derivable from formally checked definitions and theorems or it is excluded.
We build a proof-carrying metagame analytics pipeline where the verified objects are
(i) data representation and ingestion,
(ii) expected-value computations over the field,
(iii) machine-checked Nash-equilibrium computation (including all 14 pure-strategy best-response checks) and replicator-dynamics statements on real-data representative subgames, and
(iv) tournament-objective transforms (Bo3, Swiss).
The in-game rules formalization serves as supporting infrastructure guaranteeing legality and enabling future counterfactual analysis.

Our empirical foundation is Trainer Hill metagame data, aggregated from Limitless tournament records~\cite{trainerhill2026,limitless2024}, for 50+ player tournaments from January~29 to February~19, 2026.
Within this fixed window, we model the top 14 archetypes and their full pairwise matchup matrix.
The resulting matrix contains enough granularity to support rigorous expected-value computations, equilibrium analysis, and evolutionary dynamics without introducing synthetic assumptions about win rates.

This paper makes four concrete contributions aligned with this thesis.
First, we formalize the rules and legality substrate that supports trustworthy ingestion and future counterfactual analysis.
Second, we encode real metagame data as exact values and verify expected-value computations, including the popularity paradox.
Third, we derive machine-checked Nash-equilibrium statements over the full observed matrix and replicator-dynamics statements over real-data representative subgames, including complete best-response equilibrium certification.
Fourth, we verify tournament-objective transforms from single-game payoffs to best-of-three and Swiss-relevant decision criteria.

While the headline popularity paradox could be computed in a spreadsheet, the formal verification methodology provides three distinct advantages.
First, \textbf{compositional guarantees}: the Nash equilibrium certification checks best-response conditions for all 14 strategies simultaneously, a 196-cell verification that is error-prone by hand.
Second, \textbf{robustness proofs}: the worst-case bounds (Section~\ref{sec:threats}) require symbolic reasoning over parameterized win rates, not just point arithmetic.
Third, \textbf{reproducibility infrastructure}: the proof artifact serves as a machine-checkable specification that can be re-verified against updated tournament data without re-auditing the analysis logic.

The remainder of the paper is organized as follows.
Section~\ref{sec:related} situates the work.
Section~\ref{sec:formalization} presents the Lean model of rules and legality.
Section~\ref{sec:probability} develops probability and resource theory.
Section~\ref{sec:data} details data and measurement.
Section~\ref{sec:paradox} presents the popularity paradox.
Section~\ref{sec:nash} analyzes equilibrium and dynamics.
Section~\ref{sec:tournament} discusses tournament strategy.
Section~\ref{sec:methodology} documents formalization methodology.
Section~\ref{sec:threats} covers validity threats, and Section~\ref{sec:conclusion} concludes.

\subsection{Motivating Tournament Scenario}

Consider a player preparing for a 10-round open tournament.
Community sentiment says Dragapult is ``the deck to beat'' because it is popular and appears frequently in streamed matches.
Our data show why this shortcut fails: popularity is shaped by familiarity, deck cost, and social diffusion, while expected win rate is determined by pairwise matchups weighted by opponent frequencies.
These quantities are related but not equivalent, and a formal model recasts the decision as a transparent optimization problem with explicit assumptions.
Section~\ref{sec:paradox} makes this precise.

%======================================================================
\section{Related Work}\label{sec:related}
%======================================================================

\subsection{Formal Methods and Strategic Games}

Formal reasoning has transformed analysis in several strategic domains.
In classical games, Shannon's foundational analysis of chess programming and later complexity results for generalized chess established the computational stakes of strategic reasoning~\cite{shannon1950chess,schaefer1978complexity,fraenkel1981chess}.
In imperfect-information settings such as poker, game-theoretic systems like Cepheus, Libratus, and Pluribus~\cite{bowling2015heads,brown2018superhuman,brown2019pluribus} show that equilibrium reasoning can scale when abstractions are carefully managed.
At larger multi-agent scale, AlphaZero and AlphaStar demonstrate superhuman play in complex domains~\cite{silver2018general,vinyals2019alphastar}.

TCGs are harder in a different way: their compositional card interactions and exception-heavy textual semantics increase the risk of silent modeling errors, which a proof assistant mitigates by forcing explicit treatment of definitions and invariants.

\subsection{AI and Metagame Analysis in Card Games}

Prior card-game AI work has emphasized in-game decision quality, including Monte Carlo methods for games such as Magic and Hearthstone~\cite{cowling2012information,ward2009monte,santos2017monte,zhang2017deck}.
Related work also includes deckbuilding optimization and competition environments for TCG AI~\cite{bjorke2017deckbuilding,dockhorn2019hearthstone,kowalski2020summon}.
That line of work is valuable but orthogonal to the question we study: how should a player choose a deck before round one, given a population distribution and matchup matrix?

Metagame-level analysis appears frequently in practitioner content but rarely as reproducible formal scholarship.
The missing ingredients are a fixed data definition, a mathematically explicit payoff model, and mechanically checked claims.
Our approach fills this gap by treating metagame analysis as theorem proving over empirical constants.

\subsection{Theorem Proving for Rule Systems}

Lean~4~\cite{moura2021lean} has become a practical environment for large-scale formalization because it combines expressive dependent types with efficient decision procedures.
Large collaborative libraries and landmark formal proofs further show the maturity of this ecosystem~\cite{gonthier2008four,Avigad2007,mathlib2020,hales2017kepler}.
For game-rule modeling, dependent types are particularly useful: constraints such as bounded bench size, deck legality predicates, and well-formed transitions can be encoded directly as propositions attached to data.

Related work has explored formalization of card-game effects and rule systems in proof assistants~\cite{li2023card}, establishing feasibility for this class of domains.
Our work differs by coupling rule formalization to a complete, real matchup matrix and then pushing through equilibrium and dynamics claims tied to observed tournament distributions.

\subsection{Evolutionary and Behavioral Perspectives}

Replicator dynamics~\cite{smith1973logic,taylor1978evolutionary,weibull1997evolutionary} provide a natural lens for metagame adaptation: strategies with above-average fitness gain share, while below-average strategies lose share.
In practice, observed metagames often drift slowly because human behavior is not perfectly rational.
This motivates interpreting deviations from equilibrium in behavioral-economic terms rather than treating them as model failure.

The contribution here is methodological: we use evolutionary tools not as informal metaphors but as theorem-backed statements over fixed data.
When we claim that Dragapult has negative relative fitness or that Ceruledge declines under replicator updates, those claims are machine-checked consequences of encoded real-data subgame matrices and observed share vectors.

\subsection{Community Analytics Versus Proof-Carrying Analytics}

Competitive communities already produce large volumes of metagame commentary.
Those outputs are valuable for speed, but they typically combine raw percentages, subjective confidence, and hand-waved matchup transitivity.
This workflow is well suited for rapid iteration and poor at preventing silent arithmetic or modeling mistakes.

Our approach is intentionally stricter: every quantity is an explicit program term, and every strategic claim is either a verified computation or a theorem validated by the Lean kernel.
The cost is formalization overhead; the benefit is auditability and reproducibility.

This positioning is complementary.
Community analytics generate hypotheses quickly; formal analytics validate them with machine-checkable guarantees.
The popularity paradox illustrates the transformation from intuition to theorem that is the central methodological contribution.
Section~\ref{sec:formalization} now defines the formal rule substrate used by the subsequent analyses.

%======================================================================
\section{Game Formalization}\label{sec:formalization}
%======================================================================

We formalize the strategic layer of the Pok\'emon TCG by combining operational state semantics with legality and resource invariants, grounded in official rule documents~\cite{ptcg_rules,playpokemon2024rules}.
In this paper, the rules layer is supporting infrastructure rather than the primary empirical claim object for the 2026 snapshot.
It future-proofs the framework for counterfactual analysis (e.g., ``what if a card is banned?''), guarantees data-ingestion correctness through legality alignment, and enables mechanical derivation of matchup implications from legal state transitions.
The 2026 snapshot analysis itself relies primarily on matrix-level verification over observed matchup data.
The goal is not to mechanize every card text in existence; instead, we encode the rule substrate needed to reason about deck legality, turn progression, card flow, and payoff-relevant mechanics.

\subsection{Game State Representation}

The state model explicitly tracks per-player zones, turn ownership, and phase.
This representation is sufficient to express legality constraints and to prove conservation and progress properties.

\begin{lstlisting}
-- Core/Types.lean:206
structure GameState where
  player1       : PlayerState
  player2       : PlayerState
  currentPlayer : PlayerId
  turnNumber    : Nat
  phase         : TurnPhase
  stadium       : Option Card
  winner        : Option PlayerId
  deriving DecidableEq, BEq, Repr, Inhabited
\end{lstlisting}

A state-level encoding gives us a uniform target for rules, tactics, and metatheory.
In particular, all strategic claims can be connected to primitive transition semantics, preventing the common mismatch where high-level analysis assumes mechanics that are absent from the underlying rules implementation.

\subsection{Turn Phases and Transition Discipline}

Turn order is represented as a finite phase machine.
This makes ``what can happen next'' decidable, enabling automation for many local proofs.

\begin{lstlisting}
-- Core/Types.lean:181
inductive TurnPhase where
  | draw
  | main
  | attack
  | betweenTurns
  deriving DecidableEq, BEq, Repr, Inhabited
\end{lstlisting}

By constraining transitions through this type, we can prove phase safety lemmas (e.g., no attacks during draw phase) and derive stronger progress claims.
These lemmas are not just implementation detail: they ensure that all strategy-level simulations are grounded in legal game trajectories.

\subsection{Type Effectiveness}

Weakness and resistance are encoded as total functions over enumerated types.
The Grass$\to$Fire interaction appears in the certified weakness-cycle theorem.

\begin{lstlisting}
-- TypeEffectiveness.lean:138
theorem TRIANGLE :
    ∃ A B C : PType,
      weakness A B = true ∧ weakness B C = true ∧ weakness C A = true := by
  exact ⟨PType.grass, PType.fire, PType.water, rfl, rfl, rfl⟩
\end{lstlisting}

Even elementary facts matter because they serve as trusted building blocks for larger proofs, especially when computing expected damage and trade sequences.
Encoding them in the core logic avoids accidental divergence between prose assumptions and executable semantics.

\subsection{Card Conservation and Trainer Effects}

We model high-impact trainer cards with explicit zone transitions.
For Professor's Research, we prove that discarding the hand and drawing seven preserves global card count across all zones.

\begin{lstlisting}
-- CardEffects.lean:149
theorem professorsResearchEffect_preserves_cards (p : PlayerState) :
    playerCardCount (professorsResearchEffect p) = playerCardCount p := by
  unfold professorsResearchEffect playerCardCount inPlayCount
  simp [List.length_take, List.length_drop, List.length_append]
  omega
\end{lstlisting}

Conservation theorems are critical for trust.
Without them, subtle bookkeeping bugs can distort probability estimates and strategic value calculations.
With them, resource-theoretic statements in later sections inherit mechanical guarantees.

\subsection{Deck Legality as a Biconditional}

Deck legality is implemented as a computable checker and linked to an inductive specification through a soundness-and-completeness theorem.

\begin{lstlisting}
-- DeckLegality.lean:107
theorem checkDeckLegal_iff (deck : List Card) :
    checkDeckLegal deck = true ↔ DeckLegal deck := by
  constructor
  · exact checkDeckLegal_sound deck
  · exact checkDeckLegal_complete deck
\end{lstlisting}

This biconditional is essential for reproducibility.
It guarantees that data ingestion and deck filtering in empirical analysis are extensionally equivalent to the formal legality policy, rather than ``close enough'' approximations.

\subsection{Invariant Catalog and Proof Obligations}

Beyond the highlighted theorems, the formal model maintains a broad invariant catalog used by downstream analysis and simulation tooling.
Representative invariants include:
(i) nonnegative zone sizes,
(ii) deck-size preservation except under explicit draw/mill transitions,
(iii) bench-size upper bounds,
(iv) uniqueness constraints for once-per-turn actions,
(v) deterministic transition behavior under fixed randomness traces,
(vi) legality of retreat and switching operations,
(vii) prize-card accounting invariants,
(viii) terminal-state exclusivity conditions, and
(ix) well-typed status-effect transitions.

These invariants matter for metagame work because matchup payoffs are aggregate outputs of many local game interactions.
If local rule mechanics leak cards, skip phase guards, or violate zone consistency, macro-level expected values become unreliable.
The invariant layer prevents this by making such inconsistencies theorem-level failures.

A second benefit is maintainability.
As card-effect libraries expand, invariant checks serve as regression barriers.
New effect encodings must satisfy shared structural properties before they can influence strategic analysis.
This minimizes accidental model drift and keeps historical comparisons meaningful across commits.

Finally, invariants support modularity.
Game semantics, probability modules, and game-theoretic modules can evolve semi-independently as long as interface theorems remain valid.
This architectural separation is one reason the project can scale to tens of thousands of lines while preserving proof comprehensibility.
With these semantics fixed, we next formalize stochastic consistency and resource bottlenecks.

%======================================================================
\section{Probability and Resource Theory}\label{sec:probability}
%======================================================================

Strategic performance in TCGs is constrained by stochastic access (draws, coin flips, prize placement) and deterministic bottlenecks (energy attachment limits, phase restrictions).
Our Lean development captures both dimensions using exact arithmetic over rational values.

\subsection{Hypergeometric Consistency Calculations}

Opening-hand consistency follows hypergeometric structure.
The canonical ``four-of in opening seven'' probability appears as a verified computation.

\begin{lstlisting}
-- DeckConsistency.lean:49
theorem FOUR_COPIES_RULE :
    probAtLeastOne 60 4 7 = (38962 : Rat) / (97527 : Rat) ∧
    (39 : Rat) / (100 : Rat) < probAtLeastOne 60 4 7 ∧
    probAtLeastOne 60 4 7 < (2 : Rat) / (5 : Rat) := by
  decide
\end{lstlisting}

Numerically, this is approximately 39.9\%.
Likewise, with 12 Basics the no-Basic opening probability is approximately 19.1\%, and the all-four-prized event has probability $1/32{,}509$.
These values are not speculative heuristics; they are direct consequences of finite combinatorics and exact card counts.

\subsection{Energy Economy and Tempo}

One attachment per turn imposes a hard tempo cap.
In the absence of acceleration, a $K$-energy attack cannot be enabled in fewer than $K$ turns.

\begin{lstlisting}
-- EnergyEconomy.lean:37
theorem ENERGY_BOTTLENECK (K : Nat) :
    turnsToPowerUp K 0 ≥ K := by
  simp [turnsToPowerUp, attachmentsPerTurn, ceilDiv]
\end{lstlisting}

This theorem formalizes a central strategic tradeoff.
Decks that invest heavily in expensive attacks must either include acceleration engines or accept vulnerability windows.
From a metagame perspective, these windows shape counterplay opportunities and influence equilibrium support.

\subsection{Resource Theory Interpretation}

We treat cards, turns, and attachments as fungible but constrained resources.
Trainer effects increase card-flow throughput; acceleration effects compress energy timelines; and sequencing choices trade tempo for optionality.
Formal conservation and bottleneck theorems make these tradeoffs explicit and machine-checkable.

This resource view also bridges micro and macro analysis.
Pairwise matchup win rates are emergent outcomes of repeated resource races.
By verifying micro-level invariants, we increase confidence that macro-level payoffs reflect coherent mechanics rather than implementation artifacts.

\subsection{Counterfactual Resource Experiments}

The formal probability layer enables controlled counterfactuals over draw density and acceleration access, expressed as exact combinatorial models rather than Monte Carlo samples.
These tools are not directly used to modify the empirical matrix in this paper, but they support interpreting why certain archetypes occupy their observed matchup profiles and can predict how list-level adjustments propagate into metagame-level payoff shifts in future longitudinal work.
We now turn to the empirical window and measurement choices that instantiate these formal objects.

%======================================================================
\section{Tournament Data and Methodology}\label{sec:data}
%======================================================================

\subsection{Data Source and Inclusion Criteria}

All empirical values come from Trainer Hill~\cite{trainerhill2026} for Pok\'emon TCG events with at least 50 players, dates January~29 to February~19, 2026, all platforms.
Match win rates use the Trainer Hill convention
\[
\text{WR} = \frac{W + T/3}{W + L + T},
\]
where ties count as one-third of a win.
The $T/3$ weighting reflects Trainer Hill's convention that ties are worth approximately one-third of a win in Swiss standings; our robustness analysis (Section~\ref{sec:threats}) shows the headline results are insensitive to this choice.

\subsection{Modeled Archetypes and Shares}

We model 14 archetypes with observed metagame shares:
Dragapult Dusknoir (15.5\%), Gholdengo Lunatone (9.9\%), Grimmsnarl Froslass (5.1\%), Mega Absol Box (5.0\%), Gardevoir (4.6\%), Charizard Noctowl (4.3\%), Gardevoir Jellicent (4.2\%), Charizard Pidgeot (3.5\%), Dragapult Charizard (3.5\%), Raging Bolt Ogerpon (3.3\%), N's Zoroark (3.0\%), Alakazam Dudunsparce (2.8\%), Kangaskhan Bouffalant (2.5\%), and Ceruledge (2.3\%).

The top-14 aggregate is 69.5\% of the full field; the remaining 30.5\% is grouped as ``Other'' and excluded from pairwise matrix analysis.
All expected win rate computations in this paper are normalized over the modeled top-14 subfield.

\subsection{Sample Sizes and Reliability}

Critical matchup pairs are supported by large samples.
Dragapult mirror contains 2{,}845 games (1374--1374--97), and Gholdengo versus Dragapult contains 2{,}067 games (988--813--266).
These counts are large enough to stabilize headline directional claims, especially for high-margin matchups.

\subsection{Uncertainty Quantification}

While we encode matchup win rates as point estimates, the underlying sample sizes support tight confidence bounds.
We use Wilson intervals~\cite{Wilson1927} with center adjustment
\[
\tilde{p}
=
\frac{\hat{p} + z^2/(2n)}{1 + z^2/n},
\qquad
\tilde{p} \pm \frac{z}{1 + z^2/n}\sqrt{\hat{p}(1-\hat{p})/n + z^2/(4n^2)}.
\]
Both the adjusted center and the half-width include the $1/(1+z^2/n)$ factor.
For the largest matchups (e.g., Dragapult mirror: 2{,}845 games), the 95\% Wilson interval is approximately $\pm 1.8$ percentage points.
For smaller matchups (e.g., Ceruledge vs N's Zoroark: about 100 games), intervals widen to about $\pm 9$ points.
Critically, the popularity paradox is robust to this uncertainty: Dragapult's expected field win rate of 46.7\% has a 95\% interval of approximately [45.5\%, 47.9\%], entirely below 50\%, while Grimmsnarl's 52.7\% has an interval of approximately [51.0\%, 54.4\%], entirely above 50\%.
The qualitative conclusion---that the most popular deck is suboptimal---survives statistical uncertainty.

We note that the Wilson confidence intervals~\cite{Wilson1927} quantify uncertainty in individual matchup win rates but are not propagated through the expected win rate or Nash equilibrium computations. The equilibrium is verified for the point-estimate matrix; a full interval-arithmetic or bootstrapped equilibrium analysis is left to future work.

\begin{table*}[!t]
\centering
\caption{Top-6 subset view of the archetype matchup matrix (win rates \%).}
\label{tab:matchup}
\begin{tabular}{lcccccc}
\toprule
 & \textbf{Drag} & \textbf{Ghold} & \textbf{Grimm} & \textbf{Absol} & \textbf{Gard} & \textbf{Char} \\
\midrule
\textbf{Dragapult}  & 49.4 & 43.6 & 38.6 & 38.2 & 34.3 & 64.1 \\
\textbf{Gholdengo}  & 52.1 & 48.8 & 47.6 & 44.3 & 44.1 & 48.3 \\
\textbf{Grimmsnarl} & 57.2 & 46.7 & 48.5 & 34.4 & 56.6 & 55.8 \\
\textbf{Mega Absol} & 57.6 & 51.2 & 62.1 & 49.4 & 55.8 & 47.5 \\
\textbf{Gardevoir}  & 62.7 & 49.3 & 37.4 & 40.2 & 48.0 & 39.4 \\
\textbf{Charizard}  & 32.4 & 48.0 & 39.7 & 47.1 & 55.8 & 48.7 \\
\bottomrule
\end{tabular}
\end{table*}

Table~\ref{tab:matchup} uses the exact top-6 values from the Trainer Hill matrix.
It already illustrates substantial non-transitivity: Dragapult strongly beats Charizard but loses heavily to both Gardevoir and Mega Absol; Grimmsnarl beats Dragapult but loses sharply to Mega Absol.

\begin{table}[!t]
\centering
\caption{Cross-tier subset view of notable matchups (Trainer Hill, Jan--Feb 2026).}
\label{tab:crosstier}
\begin{tabular}{p{3.2cm}cp{3.1cm}}
\toprule
\textbf{Matchup} & \textbf{WR} & \textbf{Strategic reading} \\
\midrule
Raging Bolt vs Mega Absol & 67.3\% & Largest anti-Absol counter \\
Gardevoir vs Dragapult & 62.7\% & B-tier vs C-tier popular deck \\
Mega Absol vs Grimmsnarl & 62.1\% & A-tier vs S-tier \\
Dragapult vs Charizard Noctowl & 64.1\% & Popularity sustained by farm lane \\
Grimmsnarl vs Dragapult & 57.2\% & Core paradox driver \\
Raging Bolt vs Dragapult & 51.0\% & Completes the observed four-deck interaction motif \\
\bottomrule
\end{tabular}
\end{table}

Cross-tier interactions in Table~\ref{tab:crosstier} clarify why local matchup spikes do not guarantee global success.
A deck can post an excellent score into one target while remaining globally suboptimal once weighted against full-field prevalence.
These subset contrasts motivate the full-field weighted calculation used in the next section.

%======================================================================
\section{The Popularity Paradox}\label{sec:paradox}
%======================================================================

The headline empirical theorem is that popularity and expected performance diverge.
Let $s_j$ be normalized top-14 share and $w_{i,j}$ matchup win rate.
Then expected field win rate is
\[
\mathbb{E}[\mathrm{WR}_i] = \sum_j s_j\,w_{i,j}.
\]

For Dragapult Dusknoir, despite 15.5\% share, we obtain
\[
\mathbb{E}[\mathrm{WR}_{\mathrm{Dragapult}}] = 46.7\% < 50\%.
\]
For Grimmsnarl Froslass (5.1\% share), we obtain
\[
\mathbb{E}[\mathrm{WR}_{\mathrm{Grimmsnarl}}] = 52.7\%,
\]
which is the maximum among all 14 modeled decks.

\begin{lstlisting}
-- RealMetagame.lean:398
theorem dragapult_popularity_paradox :
    -- Dragapult has losing (<500) matchups against at least 8 of 14 decks
    matchupWR .DragapultDusknoir .GholdengoLunatone < 500 ∧
    matchupWR .DragapultDusknoir .GrimssnarlFroslass < 500 ∧
    matchupWR .DragapultDusknoir .MegaAbsolBox < 500 ∧
    matchupWR .DragapultDusknoir .Gardevoir < 500 ∧
    matchupWR .DragapultDusknoir .GardevoirJellicent < 500 ∧
    matchupWR .DragapultDusknoir .DragapultCharizard < 500 ∧
    matchupWR .DragapultDusknoir .RagingBoltOgerpon < 500 ∧
    matchupWR .DragapultDusknoir .NsZoroark < 500 ∧
    matchupWR .DragapultDusknoir .KangaskhanBouffalant < 500 := by decide
\end{lstlisting}

\begin{table*}[!t]
\centering
\caption{Expected win rate on the modeled top-14 subset (69.5\% of field; ``Other'' excluded). Tiers are assigned by expected field win rate: S~($\geq$52\%), A~(50--52\%), B~(48--50\%), C~($<$48\%).}
\label{tab:expected}
\begin{tabular}{lccc}
\toprule
\textbf{Archetype} & \textbf{Meta share} & \textbf{Expected WR} & \textbf{Tier} \\
\midrule
Dragapult Dusknoir & 15.5\% & 46.7\% & C \\
Gholdengo Lunatone & 9.9\% & 47.8\% & C \\
Grimmsnarl Froslass & 5.1\% & \textbf{52.7\%} & \textbf{S} \\
Mega Absol Box & 5.0\% & 51.7\% & A \\
Gardevoir & 4.6\% & 49.9\% & B \\
Charizard Noctowl & 4.3\% & 45.7\% & C \\
Gardevoir Jellicent & 4.2\% & 47.8\% & C \\
Charizard Pidgeot & 3.5\% & 46.8\% & C \\
Dragapult Charizard & 3.5\% & 48.7\% & B \\
Raging Bolt Ogerpon & 3.3\% & 47.9\% & C \\
N's Zoroark & 3.0\% & 46.9\% & C \\
Alakazam Dudunsparce & 2.8\% & 44.7\% & C \\
Kangaskhan Bouffalant & 2.5\% & 49.2\% & B \\
Ceruledge & 2.3\% & 44.8\% & C \\
\bottomrule
\end{tabular}
\end{table*}

Table~\ref{tab:expected} makes the paradox visible: the right tail of popularity is not aligned with the right tail of expected performance.

\begin{figure}[!t]
\centering
\setlength{\unitlength}{0.9mm}
\begin{picture}(88,54)
\put(10,8){\vector(1,0){72}}
\put(10,8){\vector(0,1){40}}
\put(79,2){\small Meta share (\%)}
\put(1,49){\small Exp WR (\%)}
\put(8,6){\tiny 0}
\put(31,6){\tiny 5}
\put(51,6){\tiny 10}
\put(71,6){\tiny 15}
\put(4,16){\tiny 46}
\put(4,24){\tiny 48}
\put(4,32){\tiny 50}
\put(4,40){\tiny 52}

\put(72,21){\circle*{1.8}} \put(73,22){\tiny Drag}
\put(30,42){\circle*{1.8}} \put(31,43){\tiny Grimm}
\put(30,38){\circle*{1.8}} \put(31,39){\tiny Absol}
\put(50,25){\circle*{1.8}} \put(51,26){\tiny Ghold}
\put(28,17){\circle*{1.8}} \put(29,18){\tiny Char}
\put(28,33){\circle*{1.8}} \put(29,34){\tiny Gard}
\end{picture}
\caption{Popularity paradox scatter: share versus expected win rate (top-14 normalized). Dragapult is high-share/low-fitness; Grimmsnarl is low-share/high-fitness.}
\label{fig:paradoxscatter}
\end{figure}

Figure~\ref{fig:paradoxscatter} summarizes the structural tension.
Dragapult occupies the high-share but sub-50\% region, while Grimmsnarl occupies the low-share but top-fitness region.
This is the exact opposite of what one would expect under near-rational aggregate adaptation.

\subsection{Decomposing Dragapult's Expected Fitness}

The scalar value 46.7\% is informative but opaque.
To interpret the paradox mechanically, we decompose Dragapult's expected value into prevalence-weighted contribution terms:
\[
\mathbb{E}[\mathrm{WR}_{\mathrm{Drag}}]
=
\sum_{j \in \mathcal{D}}
s_j \cdot w_{\mathrm{Drag},j}.
\]
Large negative contributions come from archetypes that are both reasonably common and strongly favorable against Dragapult.

In this snapshot, Gholdengo (9.9\% share, 43.6\% Drag WR), Gardevoir (4.6\%, 34.3\%), Grimmsnarl (5.1\%, 38.6\%), and Mega Absol (5.0\%, 38.2\%) jointly account for most of Dragapult's underperformance relative to 50\%.
Dragapult's strongest offsetting lane is Charizard Noctowl (64.1\%), but that lane alone is not enough once weighted against the rest of the field.

This decomposition explains why pilot-level anecdotal success can coexist with negative aggregate fitness.
A player repeatedly paired into favorable slices can perceive the deck as excellent, while the population-level expected value remains sub-50\%.
Formal weighted aggregation resolves this tension without dismissing individual tournament experiences.

The same decomposition also clarifies why the paradox is robust.
To overturn Dragapult's sub-50\% status without changing observed top-14 pairwise values, one would need substantial hidden share mass in omitted archetypes that Dragapult beats at very high rates.
Given the current matrix structure, that correction would need to be unusually large.

In short, the paradox is not caused by a single catastrophic matchup.
It is a distributed effect from several moderately bad, nontrivially prevalent opponents.
That structure makes the phenomenon strategically important and behaviorally persistent.

\subsection{Behavioral-Economic Interpretation}

The paradox is consistent with bounded-rationality behavior: players optimize not only expected match points but also familiarity, coordination, visibility, and card-access constraints, so adoption can remain detached from payoff-optimal choices even when weighted-EV evidence is public~\cite{Tversky1974,Kahneman1979,Banerjee1992,Bikhchandani1992}. We do not claim causal identification of these mechanisms in this dataset window; rather, we formally prove payoff-model suboptimality and treat behavioral explanations as scope-limited hypotheses for future player-level study. This separation between proven payoff statements and behavioral interpretation disciplines narrative overreach and motivates the equilibrium/dynamics analysis in Section~\ref{sec:nash}.

%======================================================================
\section{Nash Equilibrium and Metagame Dynamics}\label{sec:nash}
%======================================================================

We model deck choice as a finite two-player bimatrix game induced by the 14\,$\times$\,14 payoff matrix~\cite{NisanRoughgarden2007}.
Existence of Nash equilibria in finite games is guaranteed by Nash's theorem~\cite{nash1950equilibrium}; von Neumann's minimax theorem applies to the zero-sum special case~\cite{vonneumann1928theorie}.
Although the empirical matrix is approximately constant-sum (deviations arise from the tie convention), the Nash equilibrium is verified as a bimatrix Nash equilibrium via best-response checks for both players independently, which does not require the zero-sum assumption.
For the observed matrix, Lean now certifies a full six-deck equilibrium rather than a two-deck candidate profile.

This two-player game view is a natural approximation for head-to-head tournament matches, but it does not capture all competitive incentives.
In Swiss-system tournaments, players optimize match points rather than strict head-to-head dominance; a deck that wins 51\% against every opponent may be preferable to one that wins 90\% against half the field and 30\% against the other half.
Under a risk-averse Swiss objective (maximizing probability of reaching X-2 or better), equilibrium weight shifts toward decks with consistent, if modest, win rates.
We therefore treat this as a modeling limitation: the analysis below targets a single-match competitive benchmark, not a full Swiss-utility optimum.

\begin{lstlisting}
-- NashEquilibrium.lean:320
theorem real_nash_row_best_response_checks :
    ∀ i : Fin 14, rowPurePayoff realMetaGame14 i realNashCol ≤ realNashValue := by
  native_decide

-- NashEquilibrium.lean:324
theorem real_nash_col_best_response_checks :
    ∀ j : Fin 14, realNashValue ≤ colPurePayoff realMetaGame14 realNashRow j := by
  native_decide

-- NashEquilibrium.lean:328
theorem real_nash_equilibrium_verified :
    NashEquilibrium realMetaGame14 realNashRow realNashCol := by
  native_decide
\end{lstlisting}

Table~\ref{tab:nash} reports the verified supports for both players.
The theorem \texttt{real\_nash\_equilibrium\_verified} is backed by best-response checks quantified over all 14 pure strategies for both players, so the equilibrium claim is machine-checked rather than merely mixed-strategy-valid.
The row player's guaranteed expected payoff is \texttt{realNashValue} $= \frac{162188991282520}{338129962783} \approx 479.665$ (47.97\%).
Win rates are encoded on a 0--1000 scale, so a value of 479.67 corresponds to a 47.97\% win probability.
The nonzero entries of \texttt{realNashRowData} (row player) are at indices \{2,3,4,5,9,11\}, while the nonzero entries of \texttt{realNashColData} (column player) are at indices \{1,2,3,4,5,9\}.
Indices in Table~\ref{tab:nash} use the zero-based \texttt{Deck.toFin} mapping from \texttt{RealMetagame.lean}:
0 DragapultDusknoir, 1 GholdengoLunatone, 2 GrimssnarlFroslass, 3 MegaAbsolBox, 4 Gardevoir, 5 CharizardNoctowl, 6 GardevoirJellicent, 7 CharizardPidgeot, 8 DragapultCharizard, 9 RagingBoltOgerpon, 10 NsZoroark, 11 AlakazamDudunsparce, 12 KangaskhanBouffalant, 13 Ceruledge.

\begin{table}[!t]
\centering
\caption{Lean-verified real Nash supports for row strategy \texttt{realNashRow} and column strategy \texttt{realNashCol}.}
\label{tab:nash}
\begin{tabular}{rlcc}
\toprule
\textbf{Idx} & \textbf{Deck} & \textbf{Row weight (\texttt{realNashRow})} & \textbf{Column weight (\texttt{realNashCol})} \\
\midrule
1 & GholdengoLunatone & 0.0\% & 3.7\% \\
2 & GrimssnarlFroslass & 37.8\% & 40.5\% \\
3 & MegaAbsolBox & 12.9\% & 7.2\% \\
4 & Gardevoir & 3.5\% & 7.6\% \\
5 & CharizardNoctowl & 11.3\% & 5.0\% \\
9 & RagingBoltOgerpon & 28.7\% & 35.9\% \\
11 & AlakazamDudunsparce & 5.8\% & 0.0\% \\
\bottomrule
\end{tabular}
\end{table}

Rows in Table~\ref{tab:nash} list the union of the two six-deck supports; a 0.0\% entry indicates that deck is outside that player's support.
Table~\ref{tab:nash} shows six nonzero decks for each player, with a one-index support swap (row index 11 versus column index 1).

The row and column supports differ because the empirical matchup matrix is not perfectly antisymmetric: $M_{ij} + M_{ji} \neq 1000$ for many pairs, since win rates incorporate ties weighted as $T/3$ and arise from finite empirical samples.
This means the payoff matrix does not satisfy the constant-sum condition $M_{ij} + M_{ji} = 1000$, which would be required for identical row and column equilibrium strategies.
In a bimatrix game with such approximately-constant-sum but not exactly constant-sum structure, distinct row and column supports are mathematically expected rather than anomalous.

Crucially, Dragapult Dusknoir (15.5\% observed share) has 0\% Nash weight, which deepens the popularity paradox from Section~\ref{sec:paradox}.
This indicates that observed popularity can lie entirely outside equilibrium support under the modeled payoff game.
The Nash equilibrium is computed over the 14-archetype subgame; the remaining 30.5\% of the field (``Other'') is excluded from the strategic interaction model but accounted for in the robustness analysis (Section~\ref{sec:threats}).

Replicator dynamics formalize directional pressure~\cite{Hofbauer1998,Sandholm2010}:
\[
\dot{x}_i = x_i\left(f_i(\mathbf{x}) - \bar{f}(\mathbf{x})\right),
\qquad
\bar{f}(\mathbf{x}) = \sum_j x_j f_j(\mathbf{x}).
\]

Lean theorems \texttt{dragapult\_share\_decreases}, \texttt{ceruledge\_share\_decreases}, and \texttt{mega\_absol\_grows\_fastest} (\texttt{EvolutionaryDynamics.lean}) provide concrete machine-checked directional updates for real-data metagame subgames.
\begin{lstlisting}
-- EvolutionaryDynamics.lean:463
theorem dragapult_share_decreases :
    replicatorStep 4 realCyclePayoff realCycleMeta (1/100) ⟨0, by native_decide⟩ <
    realCycleMeta ⟨0, by native_decide⟩ := by
  optimize_proof

-- EvolutionaryDynamics.lean:665
theorem ceruledge_share_decreases :
    replicatorStep 5 realCeruPayoff realCeruMeta (1/100) ⟨4, by native_decide⟩ <
    realCeruMeta ⟨4, by native_decide⟩ := by
  optimize_proof

-- EvolutionaryDynamics.lean:523
theorem mega_absol_grows_fastest :
    replicatorStep 4 realCyclePayoff realCycleMeta (1/100) ⟨2, by native_decide⟩ -
    realCycleMeta ⟨2, by native_decide⟩ >
    replicatorStep 4 realCyclePayoff realCycleMeta (1/100) ⟨1, by native_decide⟩ -
    realCycleMeta ⟨1, by native_decide⟩ := by
  optimize_proof
\end{lstlisting}
These dynamics theorems are currently proved on representative observed-data slices (\texttt{realCyclePayoff/realCycleMeta} with 4 decks and \texttt{realCeruPayoff/realCeruMeta} with 5 decks), and there is not yet a full 14-deck replicator theorem in the artifact.
We emphasize that subgame replicator dynamics do not generally transfer to the full 14-deck game due to indirect fitness effects; these results characterize local competitive cycles within the identified subgames.
Applying these subgame results to the observed 2026 snapshot yields three verified qualitative predictions used throughout the paper:
(i) Dragapult has negative relative fitness and should lose share,
(ii) Ceruledge experiences monotone decline under repeated replicator updates,
and (iii) highest growth pressure points toward Mega Absol support growth.

\begin{figure}[!t]
\centering
\[
\begin{array}{ccc}
\text{Raging Bolt} & \xrightarrow{67.3\%} & \text{Mega Absol} \\
\downarrow_{51.0\%} & & \downarrow_{62.1\%} \\
\text{Dragapult} & \xleftarrow{57.2\%} & \text{Grimmsnarl}
\end{array}
\]
\caption{Directed metagame interaction motif with edge labels matching Table~\ref{tab:crosstier}.}
\label{fig:cycle}
\end{figure}

Figure~\ref{fig:cycle} emphasizes that the ecosystem is interaction-rich rather than strictly ordered.
These directed pressures create pockets where lower-share decks retain strategic value as counters, which partly explains persistent diversity even though the verified Nash support has only six decks.
We therefore treat replicator outputs as directional diagnostics: Dragapult pressure is downward, Ceruledge pressure is extinction-like, and Mega Absol pressure is upward when dedicated counters are scarce. These directions are falsifiable against subsequent tournament windows and provide a compact bridge from static equilibrium objects to week-to-week metagame interpretation.

%======================================================================
\section{Tournament Strategy}\label{sec:tournament}
%======================================================================

Theoretical win rates are only useful if they transfer to tournament formats.
Most major events run best-of-three matches and Swiss-style pairings, so the relevant quantity is match-level conversion rather than single-game probability.

\subsection{Best-of-Three Amplification}

For game win probability $p$, best-of-three match win probability is
$P_{\mathrm{Bo3}} = 3p^2 - 2p^3$.
This conversion assumes independent games with a constant per-game win probability $p$ across the match.
We treat this as an approximation because sideboarding and revealed-information effects can induce game-to-game dependence.
Notably, the Pok\'emon TCG does not have a sideboard mechanism, unlike games such as Magic: The Gathering, so decklists remain fixed across all games in a match; this makes the independence assumption substantially more defensible here than in other TCGs.

\begin{lstlisting}
-- TournamentStrategy.lean:31
def bo3WinProb (p : Rat) : Rat :=
  p * p * (3 - 2 * p)

-- TournamentStrategy.lean:41
def favoredRates : List Rat :=
  [11/20, 12/20, 13/20, 14/20, 15/20, 16/20, 17/20, 18/20, 19/20]

-- TournamentStrategy.lean:46
theorem BO3_AMPLIFIES_ADVANTAGE :
    ∀ p ∈ favoredRates, bo3WinProb p > p := by decide
\end{lstlisting}

\begin{table}[!t]
\centering
\caption{Bo1 to Bo3 Amplification for Key Matchups.}
\label{tab:bo3}
\begin{tabular}{lcc}
\toprule
\textbf{Matchup} & \textbf{Bo1} & \textbf{Bo3} \\
\midrule
Raging Bolt vs Mega Absol & 67.3\% & 74.9\% \\
Gardevoir vs Dragapult & 62.7\% & 68.6\% \\
Mega Absol vs Grimmsnarl & 62.1\% & 67.8\% \\
Grimmsnarl vs Dragapult & 57.2\% & 60.7\% \\
Dragapult vs Charizard Noctowl & 64.1\% & 70.6\% \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:bo3} shows that the 67.3\%\,$\to$\,74.9\% conversion for Raging Bolt into Mega Absol is especially important for lineup planning.
Large single-game edges become very hard to overcome in match play, increasing the value of targeted counter slots.

\subsection{Swiss Considerations}

Swiss tournaments reward consistency and resilience to bad pairings.
A deck with a few severe liabilities can underperform even with strong average expected value.
Therefore, in addition to maximizing expected WR, one should minimize exposure to high-share bad matchups and evaluate cut-line probabilities under realistic field distributions~\cite{romero2022swiss}.
For an eight-round Swiss event with an X--2 qualification target and field-level Bo3 win probability $p_m$, the cut-line transform is
$P(\text{X--2 or better}) = \sum_{k=6}^{8} \binom{8}{k} p_m^k (1-p_m)^{8-k}$.
In practice, registration should combine projected field shares, weighted expected WR, and explicit stress tests on the largest counter-edges (especially Mega Absol versus Raging Bolt) rather than rely on average EV alone.

%======================================================================
\section{Formalization Methodology}\label{sec:methodology}
%======================================================================

Our methodology prioritizes proof transparency, reproducibility, and empirical traceability, following large-scale formalization practice in Lean and related projects~\cite{mathlib2020,Buzzard2020}.
Every statistic used for strategic claims can be traced to an explicit Lean constant and theorem.
Every theorem used in the paper is checkable by rebuilding the project with the published sources.

\subsection{Zero-Axiom, Zero-Sorry Standard}

The development enforces a strict policy: no \texttt{sorry}, no \texttt{admit}, and no custom axioms, turning persuasive-but-brittle metagame narratives into inspectable, machine-checked artifacts.

\subsection{Proof Engineering Pattern}

Most proofs in this work follow one of four patterns:
(1) decision procedures over finite domains (\texttt{native\_decide}, \texttt{decide}),
(2) arithmetic normalization (\texttt{omega}, \texttt{nlinarith}),
(3) definitional unfolding and rewriting (\texttt{simp}), and
(4) decomposition of strategic statements into finite conjunctions over concrete decks.
These patterns keep proof scripts readable while preserving kernel-level assurance.

\subsection{Trust Boundary: \texttt{native\_decide}}

Several key theorems --- including the Nash equilibrium best-response checks and some replicator-dynamics results --- rely on \texttt{native\_decide}, which compiles Lean decision procedures to native code and executes them outside the kernel's proof-term checker.
We use \texttt{native\_decide} for computational verification. While this bypasses the kernel proof-term checker, it is standard practice in the Lean and Mathlib communities for large finite decision problems where \texttt{decide} would require prohibitive compilation time or memory.
We verified a representative subset of key theorems with kernel-checked \texttt{decide} to validate consistency.
All uses of \texttt{native\_decide} in this development could in principle be replaced by \texttt{decide} at the cost of substantially longer build times.

\subsection{Module-Level Statistics}

\begin{table*}[!t]
\centering
\caption{Formalization Module Breakdown.}
\label{tab:modules}
\begin{tabular}{lrrr}
\toprule
\textbf{Module group} & \textbf{Files} & \textbf{Lines} & \textbf{Theorems} \\
\midrule
Core Rules \& Semantics & 10 & 4{,}097 & 253 \\
Card Effects \& Actions & 8 & 3{,}055 & 171 \\
Probability \& Resources & 8 & 2{,}522 & 176 \\
Game Theory \& Dynamics & 6 & 3{,}562 & 350 \\
Real Metagame Analysis & 6 & 2{,}013 & 179 \\
Infrastructure \& Validation & 7 & 1{,}872 & 97 \\
Additional Specialized Modules & 30 & 12{,}672 & 1{,}284 \\
\midrule
\textbf{Total} & \textbf{75} & \textbf{29{,}793} & \textbf{2{,}510} \\
\bottomrule
\end{tabular}
\end{table*}

Table~\ref{tab:modules} reports module-level code statistics used in this project snapshot.
Counts are from the current Lean tree and include files, lines, and theorem/lemma/example declarations (obtained via \texttt{wc~-l} and \texttt{grep~-c~'theorem\textbackslash|lemma'} across all \texttt{.lean} files).
Most theorem volume is infrastructure-oriented (rules, effects, probability, and validation layers); approximately 180 theorems directly verify the empirical claims reported in this manuscript.

\subsection{Reproducibility Workflow}

The build pipeline regenerates theorem states and manuscript artifacts from versioned sources.
Data tables in the manuscript match constants in \texttt{RealMetagame.lean} and \texttt{MatchupAnalysis.lean}; key strategic claims are mirrored by named theorems.
This one-to-one mapping sharply reduces the risk of drift between code and prose.
A Python script can recompute percentages quickly, but it does not enforce theorem-level linkage between assumptions, constants, and manuscript claims.
The Lean pipeline adds that linkage and fails loudly when any claim drifts from its formal source.

\subsection{Human Review and Artifact Audit}

Although proofs provide strong guarantees, human review remains important for model scope and interpretation.
Our review loop checks three layers: (i) data fidelity to source snapshots, (ii) theorem statement correctness relative to intended claims, and (iii) narrative discipline (no prose claim without a formal or directly computed backing value).

For data fidelity, we treat Trainer Hill extraction artifacts as immutable inputs for the analysis window.
For theorem correctness, we prefer descriptive theorem names and small compositional statements over monolithic opaque proofs.
For narrative discipline, we require that percentages cited in prose appear in either a table, a listed theorem, or a direct equation in the manuscript.

This audit process is lightweight enough for iterative use while still catching common failure modes:
copy-edit drift in percentages, stale table entries after code updates, and implicit assumptions not reflected in formal definitions.
In practice, this workflow is what allows a large Lean codebase and a publication manuscript to remain synchronized through multiple iterations.

\subsection{Case Study: Verifying a Headline Claim End-to-End}

To illustrate traceability, consider the statement:
``Dragapult is 15.5\% of the meta but only 46.7\% expected against the field.''
In our workflow, this statement is decomposed into auditable steps:
\begin{enumerate}
\item Extract share constants from the fixed Trainer Hill snapshot.
\item Normalize top-14 shares for expected-value computation.
\item Compute weighted expectation from the encoded 14\,$\times$\,14 matrix.
\item Express the result as an exact rational in Lean.
\item Prove the inequality relative to 50\% in theorem form.
\item Reuse the same constants in manuscript tables and prose.
\end{enumerate}

Each step is versioned and reproducible.
If any upstream value changes (for example, an updated matrix entry after a data correction), the downstream theorem or table regeneration will fail or produce a different value, making drift explicit.
This is fundamentally stronger than spreadsheet-style pipelines where hidden references and manual edits can silently desynchronize outputs.

The same pattern is used for all other headline claims in this paper:
Grimmsnarl's top expected value, the machine-checked six-deck Nash equilibrium (including zero Dragapult support), Raging Bolt's 67.3\% counter edge, and Bo3 amplification values.
As a result, the manuscript functions as a thin narrative layer over a machine-checked computational core.

\subsection{Continuous Metagame Monitoring}

The infrastructure naturally supports continuous operation: new tournament snapshots can be ingested, Lean constants rebuilt, and theorem checks re-run, with unchanged claims remaining certified and changed claims failing loudly.
Because modules are factored by responsibility, updates can target only affected components, keeping turnaround practical for weekly competitive cycles.
We next delimit the empirical and modeling bounds of these results in Section~\ref{sec:threats}.

%======================================================================
\section{Threats to Validity}\label{sec:threats}
%======================================================================

\textbf{Temporal locality.}
The analyzed window is three weeks.
Metagames can shift rapidly due to innovation, counter-adaptation, and card availability.
Our claims describe this window precisely; they are not universal constants.
However, temporal locality is not purely a weakness: short windows reduce hidden confounding from major ruleset changes.
Future work should combine rolling windows with change-point detection to separate genuine adaptation from transient noise.

\textbf{Top-14 normalization.}
Expected win rates are normalized over the modeled 69.5\% top-14 subset.
A different treatment of the 30.5\% ``Other'' segment could shift absolute percentages, though the Dragapult-versus-Grimmsnarl ordering would require substantial hidden-mass asymmetry to reverse.
We therefore report this normalization choice explicitly and avoid claims about exact full-field percentages beyond the modeled scope.

\subsection{Robustness Analysis}

A natural concern is whether the unmodeled 30.5\% of the field could reverse our conclusions.
We provide machine-checked worst-case bounds.
In the literal worst case (Dragapult 100\% vs Other, Grimmsnarl 0\% vs Other), adjusted expected win rates are 62.9\% and 36.6\%.
Dragapult requires at least 57.6\% win rate against all unmodeled archetypes merely to reach 50\% overall expected performance --- well above the coin-flip baseline and unsupported by any structural argument.
Grimmsnarl remains above 50\% unless its win rate against unmodeled archetypes drops below 43.9\%, a scenario inconsistent with its favorable type coverage.
Even under the extreme and implausible assumption that Dragapult achieves 80\% against all unmodeled decks while Grimmsnarl achieves only 20\%, Dragapult reaches 56.8\% and Grimmsnarl drops to 42.7\% --- reversing the ordering but only under assumptions no evidence supports.
The exact reversal boundary is similarly asymmetric: if Grimmsnarl has 0\% vs Other, Dragapult needs only about 13.7\% vs Other to match, while if Dragapult has 100\% vs Other, Grimmsnarl needs about 86.3\% to match.
The paradox is robust.

We also verify robustness to meta share perturbation.
The popularity paradox arises from Dragapult's matchup spread --- it loses to 9 of 13 non-mirror opponents --- rather than from any particular share distribution.
Machine-checked share-perturbation theorems in \texttt{SharePerturbation.lean} show that even if Dragapult's share drops from 15.5\% to 5\%, its expected field win rate remains below 50\%; conversely, if Grimmsnarl's share rises from 5.1\% to 15.5\% (absorbing Dragapult's current share), its expected field win rate remains above 50\%.
The paradox is structural: it derives from the matchup matrix, not the share vector, so the central finding is not an artifact of a particular metagame snapshot but reflects a persistent property of the underlying strategic landscape.

\begin{lstlisting}
-- Robustness.lean:54
theorem popularity_paradox_robust_worst_case :
    adjustedWR dragapultTop14WR 1 = 629243 / 1000000 ∧
    adjustedWR grimmsnarlTop14WR 0 = 366061 / 1000000 ∧
    adjustedWR dragapultTop14WR 1 - adjustedWR grimmsnarlTop14WR 0 = 131591 / 500000 := by
  constructor
  · decide
  constructor
  · decide
  · decide
\end{lstlisting}

\textbf{Archetype granularity.}
Each archetype is treated as a point strategy.
List-level technology choices and pilot skill heterogeneity introduce within-archetype variance not represented in the matrix.
This is a standard abstraction tradeoff: coarse archetype bins improve statistical power but hide intra-bin adaptation.
A natural extension is hierarchical modeling with sub-archetype clusters once sample sizes permit.

\textbf{Strategic objective mismatch.}
Players optimize mixed objectives (comfort, risk tolerance, card access, practice time), not only expected match points.
Observed non-equilibrium play can therefore be rational under private utility functions even when suboptimal under public payoff assumptions.
Our ``suboptimal'' terminology is therefore always relative to the stated payoff model, not a universal claim about all player preferences.
With these limitations explicit, we summarize the main findings and concrete next steps in Section~\ref{sec:conclusion}.

%======================================================================
\section{Conclusion}\label{sec:conclusion}
%======================================================================

This paper presents a formally verified metagame analysis pipeline for a real competitive TCG environment.
Using Lean~4 plus Trainer Hill data, we prove a popularity paradox (Section~\ref{sec:paradox}) and connect it to equilibrium, dynamics, and tournament implications.

We also connect static matchup structure to dynamic and tournament implications.
The Lean-verified real Nash equilibrium has six-deck support per player and 0\% Dragapult weight; on representative observed-data subgames (4-deck and 5-deck slices, not the full 14-deck matrix), replicator dynamics predict Dragapult decline and Ceruledge extinction pressure, though these subgame results do not generally transfer to the full game due to indirect fitness effects; and best-of-three math amplifies already-large matchup edges (67.3\%\,$\to$\,74.9\%).

Immediate next steps are concrete: (i) extend the dataset to rolling weekly windows with uncertainty intervals, enabling forecast calibration by scoring each window's predictions against the next, (ii) model the 30.5\% ``Other'' segment explicitly instead of excluding it from matrix analysis, and (iii) add hierarchical sub-archetype clustering to capture list-level variance and pilot heterogeneity.

\subsection{Broader Implications for Competitive Game Science}

This case study suggests a general template for competitive-game research.
First, formalize core mechanics and legality.
Second, encode empirical payoff data as exact values.
Third, express strategic claims as theorem-checkable statements.
Fourth, tie those claims to tournament-relevant objectives instead of abstract utility alone.
The resulting pipeline is portable across many environments with discrete strategy choices and measurable outcomes.

The portability argument matters because many competitive ecosystems face the same methodological failure mode:
high-quality data exist, but conclusions are often produced by ad hoc tooling that mixes assumptions and results without explicit traceability.
A proof-assisted workflow does not replace domain expertise; it structures it.
Experts still decide which assumptions are reasonable, but once assumptions are fixed, conclusions become machine-auditable rather than rhetorical.

A second implication concerns collaboration between researchers and practitioners.
Formal artifacts can be integrated into testing-team workflows as ``verified baselines'' against which local innovations are evaluated.
For example, a team can begin from a certified weighted matchup model, then test whether a candidate list change moves specific matchup entries enough to alter tier or equilibrium-relevant conclusions.
This is far more informative than relying on isolated scrim records without a stable analytical backbone.

Finally, this work contributes to a broader view of theorem proving in applied settings.
Proof assistants are often associated with pure mathematics or compiler correctness.
Our results show they are also practical for empirical strategic science when the domain provides structured, finite data and well-defined objective functions.
In that regime, formal methods can simultaneously improve reproducibility, interpretability, and decision quality.

Beyond this specific metagame snapshot, the broader contribution is methodological.
Formal verification can serve as a practical scientific instrument for competitive game ecosystems: it turns qualitative metagame claims into executable definitions, theorem statements, and reproducible evidence.

\section*{Data Availability}
Data were extracted from Trainer Hill (trainerhill.com) on February~19, 2026, filtering for Pok\'emon TCG events with 50 or more players between January~29 and February~19, 2026.
Trainer Hill aggregates results from the Limitless TCG tournament platform.
The 14\,$\times$\,14 matchup matrix was computed from win-loss-tie records; ties were weighted as one-third of a win following the standard conversion $\mathrm{WR} = (W + T/3)/(W + L + T)$.
The complete matrix with raw W-L-T counts is archived in the repository.
All Lean~4 source code, data files, and build instructions will be made publicly available upon acceptance in an anonymized artifact.

\balance
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
